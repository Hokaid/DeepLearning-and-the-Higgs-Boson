{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trabajo Parcial - Data Mining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hokaid/DeepLearning-and-the-Higgs-Boson/blob/main/DeepLearningProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCpsJUfbx_HR"
      },
      "source": [
        "#***Students***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POG9ej_2yMLn"
      },
      "source": [
        "- ***Geral Esteen Castillo Arredondo (U201716913)***\n",
        "\n",
        "- ***Arian Yturrizaga\n",
        "(U201716372)***\n",
        "\n",
        "- ***Franco Moloche (U201715100)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCJJuq6E73uW"
      },
      "source": [
        "#***I. Reading and Description of the data***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m75zCm1j8GVs"
      },
      "source": [
        "##***1. Data reading***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGi4Qd-bM81z"
      },
      "source": [
        "In the following code cells, the respective data set is read from google drive. The corresponding file to be read has the name ***training.csv***. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JzlCRcf7_Uq"
      },
      "source": [
        "#Read a CSV\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials \n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQQi7hHwNr1K",
        "outputId": "2c69f23e-e51c-4d4c-cd51-31e0c3f5c6eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import data_table\n",
        "\n",
        "link_google_drive = 'https://drive.google.com/open?id=1skak0UEfIhU3GkDl7V7hTyIaxDb-KMQ-'\n",
        "flu, id = link_google_drive.split('=')\n",
        "dataset = drive.CreateFile({'id':id})\n",
        "dataset.GetContentFile('training.csv')\n",
        "df = pd.read_csv('training.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EventId</th>\n",
              "      <th>DER_mass_MMC</th>\n",
              "      <th>DER_mass_transverse_met_lep</th>\n",
              "      <th>DER_mass_vis</th>\n",
              "      <th>DER_pt_h</th>\n",
              "      <th>DER_deltaeta_jet_jet</th>\n",
              "      <th>DER_mass_jet_jet</th>\n",
              "      <th>DER_prodeta_jet_jet</th>\n",
              "      <th>DER_deltar_tau_lep</th>\n",
              "      <th>DER_pt_tot</th>\n",
              "      <th>DER_sum_pt</th>\n",
              "      <th>DER_pt_ratio_lep_tau</th>\n",
              "      <th>DER_met_phi_centrality</th>\n",
              "      <th>DER_lep_eta_centrality</th>\n",
              "      <th>PRI_tau_pt</th>\n",
              "      <th>PRI_tau_eta</th>\n",
              "      <th>PRI_tau_phi</th>\n",
              "      <th>PRI_lep_pt</th>\n",
              "      <th>PRI_lep_eta</th>\n",
              "      <th>PRI_lep_phi</th>\n",
              "      <th>PRI_met</th>\n",
              "      <th>PRI_met_phi</th>\n",
              "      <th>PRI_met_sumet</th>\n",
              "      <th>PRI_jet_num</th>\n",
              "      <th>PRI_jet_leading_pt</th>\n",
              "      <th>PRI_jet_leading_eta</th>\n",
              "      <th>PRI_jet_leading_phi</th>\n",
              "      <th>PRI_jet_subleading_pt</th>\n",
              "      <th>PRI_jet_subleading_eta</th>\n",
              "      <th>PRI_jet_subleading_phi</th>\n",
              "      <th>PRI_jet_all_pt</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100000</td>\n",
              "      <td>138.470</td>\n",
              "      <td>51.655</td>\n",
              "      <td>97.827</td>\n",
              "      <td>27.980</td>\n",
              "      <td>0.91</td>\n",
              "      <td>124.711</td>\n",
              "      <td>2.666</td>\n",
              "      <td>3.064</td>\n",
              "      <td>41.928</td>\n",
              "      <td>197.760</td>\n",
              "      <td>1.582</td>\n",
              "      <td>1.396</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32.638</td>\n",
              "      <td>1.017</td>\n",
              "      <td>0.381</td>\n",
              "      <td>51.626</td>\n",
              "      <td>2.273</td>\n",
              "      <td>-2.414</td>\n",
              "      <td>16.824</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>258.733</td>\n",
              "      <td>2</td>\n",
              "      <td>67.435</td>\n",
              "      <td>2.150</td>\n",
              "      <td>0.444</td>\n",
              "      <td>46.062</td>\n",
              "      <td>1.24</td>\n",
              "      <td>-2.475</td>\n",
              "      <td>113.497</td>\n",
              "      <td>0.002653</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100001</td>\n",
              "      <td>160.937</td>\n",
              "      <td>68.768</td>\n",
              "      <td>103.235</td>\n",
              "      <td>48.146</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.473</td>\n",
              "      <td>2.078</td>\n",
              "      <td>125.157</td>\n",
              "      <td>0.879</td>\n",
              "      <td>1.414</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>42.014</td>\n",
              "      <td>2.039</td>\n",
              "      <td>-3.011</td>\n",
              "      <td>36.918</td>\n",
              "      <td>0.501</td>\n",
              "      <td>0.103</td>\n",
              "      <td>44.704</td>\n",
              "      <td>-1.916</td>\n",
              "      <td>164.546</td>\n",
              "      <td>1</td>\n",
              "      <td>46.226</td>\n",
              "      <td>0.725</td>\n",
              "      <td>1.158</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>46.226</td>\n",
              "      <td>2.233584</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100002</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>162.172</td>\n",
              "      <td>125.953</td>\n",
              "      <td>35.635</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.148</td>\n",
              "      <td>9.336</td>\n",
              "      <td>197.814</td>\n",
              "      <td>3.776</td>\n",
              "      <td>1.414</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>32.154</td>\n",
              "      <td>-0.705</td>\n",
              "      <td>-2.093</td>\n",
              "      <td>121.409</td>\n",
              "      <td>-0.953</td>\n",
              "      <td>1.052</td>\n",
              "      <td>54.283</td>\n",
              "      <td>-2.186</td>\n",
              "      <td>260.414</td>\n",
              "      <td>1</td>\n",
              "      <td>44.251</td>\n",
              "      <td>2.053</td>\n",
              "      <td>-2.028</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>44.251</td>\n",
              "      <td>2.347389</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100003</td>\n",
              "      <td>143.905</td>\n",
              "      <td>81.417</td>\n",
              "      <td>80.943</td>\n",
              "      <td>0.414</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.310</td>\n",
              "      <td>0.414</td>\n",
              "      <td>75.968</td>\n",
              "      <td>2.354</td>\n",
              "      <td>-1.285</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>22.647</td>\n",
              "      <td>-1.655</td>\n",
              "      <td>0.010</td>\n",
              "      <td>53.321</td>\n",
              "      <td>-0.522</td>\n",
              "      <td>-3.100</td>\n",
              "      <td>31.082</td>\n",
              "      <td>0.060</td>\n",
              "      <td>86.062</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>5.446378</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100004</td>\n",
              "      <td>175.864</td>\n",
              "      <td>16.915</td>\n",
              "      <td>134.805</td>\n",
              "      <td>16.405</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.891</td>\n",
              "      <td>16.405</td>\n",
              "      <td>57.983</td>\n",
              "      <td>1.056</td>\n",
              "      <td>-1.385</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>28.209</td>\n",
              "      <td>-2.197</td>\n",
              "      <td>-2.231</td>\n",
              "      <td>29.774</td>\n",
              "      <td>0.798</td>\n",
              "      <td>1.569</td>\n",
              "      <td>2.723</td>\n",
              "      <td>-0.871</td>\n",
              "      <td>53.131</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>6.245333</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   EventId  DER_mass_MMC  ...    Weight  Label\n",
              "0   100000       138.470  ...  0.002653      s\n",
              "1   100001       160.937  ...  2.233584      b\n",
              "2   100002      -999.000  ...  2.347389      b\n",
              "3   100003       143.905  ...  5.446378      b\n",
              "4   100004       175.864  ...  6.245333      b\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq_HzaK7Mb9f"
      },
      "source": [
        "##***2. Description of the dataset***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV30YTLHRLzp"
      },
      "source": [
        "***Dataset context:*** The dataset is about detection of the Higgs Boson in simulated data in order to reproduce the behavior of the ATLAS experience. This is a binary classification problem, or event detection. Now, we will proceed to define the class or objective attribute: \n",
        "\n",
        "* ***Label:*** It indicates ***s*** when the class is ***tau tau decay of a Higgs boson***. If it indicates ***b***, means that the class is ***Background***. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9xRVjTqzLdh"
      },
      "source": [
        "#***II. Data preprocessing***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMCreE-gSrGR"
      },
      "source": [
        "##***1. Cleaning and data processing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgXl0XGDT8lu"
      },
      "source": [
        "###***A. Elimination of duplicated data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnEX2VpLWpa2"
      },
      "source": [
        "Redundant data that can be found in the data must be eliminated, since having duplicated data does not represent any gain in information. The following code is used to eliminate duplicated data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4SYWTuNWqjq"
      },
      "source": [
        "#Eliminate duplicated data\n",
        "df = df.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0dxjgedWLWi"
      },
      "source": [
        "Now, it will be checked that there are no duplicate records with the following code, which prints the number of duplicate records in the data set.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaJvzQ6d1G30",
        "outputId": "3f930a8b-a540-4764-e268-42eb9b3326e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#identificar si hay valores duplicados\n",
        "ddf = df.duplicated()\n",
        "n_vdupli = 0 #numeros de valores duplicados\n",
        "for i in range(len(ddf)):\n",
        "  if (ddf[i] == True):\n",
        "    n_vdupli += 1\n",
        "print(n_vdupli)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKZr4soxrB_K"
      },
      "source": [
        "###***B. Treatment of missing values***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuKrJvhS2XqP"
      },
      "source": [
        "The missing values ​​are then printed for each column. In this case, we can identify that there are no missing values ​​in the data set.\n",
        "\n",
        "***Conclusion: it was not possible to identify null values ​​in the given data.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eUpyIPbYzXJ",
        "outputId": "f40417f5-176d-435c-d88d-be43cd418ef4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        " print(\"number of records: \", len(df))\n",
        " df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of records:  250000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EventId                        0\n",
              "DER_mass_MMC                   0\n",
              "DER_mass_transverse_met_lep    0\n",
              "DER_mass_vis                   0\n",
              "DER_pt_h                       0\n",
              "DER_deltaeta_jet_jet           0\n",
              "DER_mass_jet_jet               0\n",
              "DER_prodeta_jet_jet            0\n",
              "DER_deltar_tau_lep             0\n",
              "DER_pt_tot                     0\n",
              "DER_sum_pt                     0\n",
              "DER_pt_ratio_lep_tau           0\n",
              "DER_met_phi_centrality         0\n",
              "DER_lep_eta_centrality         0\n",
              "PRI_tau_pt                     0\n",
              "PRI_tau_eta                    0\n",
              "PRI_tau_phi                    0\n",
              "PRI_lep_pt                     0\n",
              "PRI_lep_eta                    0\n",
              "PRI_lep_phi                    0\n",
              "PRI_met                        0\n",
              "PRI_met_phi                    0\n",
              "PRI_met_sumet                  0\n",
              "PRI_jet_num                    0\n",
              "PRI_jet_leading_pt             0\n",
              "PRI_jet_leading_eta            0\n",
              "PRI_jet_leading_phi            0\n",
              "PRI_jet_subleading_pt          0\n",
              "PRI_jet_subleading_eta         0\n",
              "PRI_jet_subleading_phi         0\n",
              "PRI_jet_all_pt                 0\n",
              "Weight                         0\n",
              "Label                          0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz5jPgTF6a5z"
      },
      "source": [
        "###***C. Transformation of categorical data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrOVdII_6y7D"
      },
      "source": [
        "The ***Label*** column, which is the class column of the dataset, was represented in a categorical way. What had to be done was to transform it to numeric, so that it can be used by the model chosen later. Most learning models only understand numerical data. In that sense, it is appropriate to code the categorical variables of the data set. Then the specified is done. The first 5 instances of the data set are printed with the categorical variables coded to numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGpH8wSb38X5",
        "outputId": "657f67b4-9c40-43db-a644-00ba1a624adf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "#Codificando todas las variables categoricas, ya que los clasificadores solo entienden datos numericos\n",
        "categorical_feature_mask = df.dtypes==object\n",
        "categorical_cols = df.columns[categorical_feature_mask].tolist()\n",
        "le = LabelEncoder()\n",
        "df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EventId</th>\n",
              "      <th>DER_mass_MMC</th>\n",
              "      <th>DER_mass_transverse_met_lep</th>\n",
              "      <th>DER_mass_vis</th>\n",
              "      <th>DER_pt_h</th>\n",
              "      <th>DER_deltaeta_jet_jet</th>\n",
              "      <th>DER_mass_jet_jet</th>\n",
              "      <th>DER_prodeta_jet_jet</th>\n",
              "      <th>DER_deltar_tau_lep</th>\n",
              "      <th>DER_pt_tot</th>\n",
              "      <th>DER_sum_pt</th>\n",
              "      <th>DER_pt_ratio_lep_tau</th>\n",
              "      <th>DER_met_phi_centrality</th>\n",
              "      <th>DER_lep_eta_centrality</th>\n",
              "      <th>PRI_tau_pt</th>\n",
              "      <th>PRI_tau_eta</th>\n",
              "      <th>PRI_tau_phi</th>\n",
              "      <th>PRI_lep_pt</th>\n",
              "      <th>PRI_lep_eta</th>\n",
              "      <th>PRI_lep_phi</th>\n",
              "      <th>PRI_met</th>\n",
              "      <th>PRI_met_phi</th>\n",
              "      <th>PRI_met_sumet</th>\n",
              "      <th>PRI_jet_num</th>\n",
              "      <th>PRI_jet_leading_pt</th>\n",
              "      <th>PRI_jet_leading_eta</th>\n",
              "      <th>PRI_jet_leading_phi</th>\n",
              "      <th>PRI_jet_subleading_pt</th>\n",
              "      <th>PRI_jet_subleading_eta</th>\n",
              "      <th>PRI_jet_subleading_phi</th>\n",
              "      <th>PRI_jet_all_pt</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100000</td>\n",
              "      <td>138.470</td>\n",
              "      <td>51.655</td>\n",
              "      <td>97.827</td>\n",
              "      <td>27.980</td>\n",
              "      <td>0.91</td>\n",
              "      <td>124.711</td>\n",
              "      <td>2.666</td>\n",
              "      <td>3.064</td>\n",
              "      <td>41.928</td>\n",
              "      <td>197.760</td>\n",
              "      <td>1.582</td>\n",
              "      <td>1.396</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32.638</td>\n",
              "      <td>1.017</td>\n",
              "      <td>0.381</td>\n",
              "      <td>51.626</td>\n",
              "      <td>2.273</td>\n",
              "      <td>-2.414</td>\n",
              "      <td>16.824</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>258.733</td>\n",
              "      <td>2</td>\n",
              "      <td>67.435</td>\n",
              "      <td>2.150</td>\n",
              "      <td>0.444</td>\n",
              "      <td>46.062</td>\n",
              "      <td>1.24</td>\n",
              "      <td>-2.475</td>\n",
              "      <td>113.497</td>\n",
              "      <td>0.002653</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100001</td>\n",
              "      <td>160.937</td>\n",
              "      <td>68.768</td>\n",
              "      <td>103.235</td>\n",
              "      <td>48.146</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.473</td>\n",
              "      <td>2.078</td>\n",
              "      <td>125.157</td>\n",
              "      <td>0.879</td>\n",
              "      <td>1.414</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>42.014</td>\n",
              "      <td>2.039</td>\n",
              "      <td>-3.011</td>\n",
              "      <td>36.918</td>\n",
              "      <td>0.501</td>\n",
              "      <td>0.103</td>\n",
              "      <td>44.704</td>\n",
              "      <td>-1.916</td>\n",
              "      <td>164.546</td>\n",
              "      <td>1</td>\n",
              "      <td>46.226</td>\n",
              "      <td>0.725</td>\n",
              "      <td>1.158</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>46.226</td>\n",
              "      <td>2.233584</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100002</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>162.172</td>\n",
              "      <td>125.953</td>\n",
              "      <td>35.635</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.148</td>\n",
              "      <td>9.336</td>\n",
              "      <td>197.814</td>\n",
              "      <td>3.776</td>\n",
              "      <td>1.414</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>32.154</td>\n",
              "      <td>-0.705</td>\n",
              "      <td>-2.093</td>\n",
              "      <td>121.409</td>\n",
              "      <td>-0.953</td>\n",
              "      <td>1.052</td>\n",
              "      <td>54.283</td>\n",
              "      <td>-2.186</td>\n",
              "      <td>260.414</td>\n",
              "      <td>1</td>\n",
              "      <td>44.251</td>\n",
              "      <td>2.053</td>\n",
              "      <td>-2.028</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>44.251</td>\n",
              "      <td>2.347389</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100003</td>\n",
              "      <td>143.905</td>\n",
              "      <td>81.417</td>\n",
              "      <td>80.943</td>\n",
              "      <td>0.414</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.310</td>\n",
              "      <td>0.414</td>\n",
              "      <td>75.968</td>\n",
              "      <td>2.354</td>\n",
              "      <td>-1.285</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>22.647</td>\n",
              "      <td>-1.655</td>\n",
              "      <td>0.010</td>\n",
              "      <td>53.321</td>\n",
              "      <td>-0.522</td>\n",
              "      <td>-3.100</td>\n",
              "      <td>31.082</td>\n",
              "      <td>0.060</td>\n",
              "      <td>86.062</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>5.446378</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100004</td>\n",
              "      <td>175.864</td>\n",
              "      <td>16.915</td>\n",
              "      <td>134.805</td>\n",
              "      <td>16.405</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.891</td>\n",
              "      <td>16.405</td>\n",
              "      <td>57.983</td>\n",
              "      <td>1.056</td>\n",
              "      <td>-1.385</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>28.209</td>\n",
              "      <td>-2.197</td>\n",
              "      <td>-2.231</td>\n",
              "      <td>29.774</td>\n",
              "      <td>0.798</td>\n",
              "      <td>1.569</td>\n",
              "      <td>2.723</td>\n",
              "      <td>-0.871</td>\n",
              "      <td>53.131</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>6.245333</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   EventId  DER_mass_MMC  ...    Weight  Label\n",
              "0   100000       138.470  ...  0.002653      1\n",
              "1   100001       160.937  ...  2.233584      0\n",
              "2   100002      -999.000  ...  2.347389      0\n",
              "3   100003       143.905  ...  5.446378      0\n",
              "4   100004       175.864  ...  6.245333      0\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpMv5KbDHyEZ"
      },
      "source": [
        "###***D. Data reduction***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBzDy28yH-da"
      },
      "source": [
        "It was analyzed that the column ***EventId*** was basically the ID for each record, and it was considered irrelevant to work the model, so this column was discarded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y5fqJFtI3IQ",
        "outputId": "0236267e-6cac-43ae-b0dc-23e0ce306fea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "#dropped the eventid column because it's irrelevant\n",
        "df = df.drop([\"EventId\"], axis = 1)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DER_mass_MMC</th>\n",
              "      <th>DER_mass_transverse_met_lep</th>\n",
              "      <th>DER_mass_vis</th>\n",
              "      <th>DER_pt_h</th>\n",
              "      <th>DER_deltaeta_jet_jet</th>\n",
              "      <th>DER_mass_jet_jet</th>\n",
              "      <th>DER_prodeta_jet_jet</th>\n",
              "      <th>DER_deltar_tau_lep</th>\n",
              "      <th>DER_pt_tot</th>\n",
              "      <th>DER_sum_pt</th>\n",
              "      <th>DER_pt_ratio_lep_tau</th>\n",
              "      <th>DER_met_phi_centrality</th>\n",
              "      <th>DER_lep_eta_centrality</th>\n",
              "      <th>PRI_tau_pt</th>\n",
              "      <th>PRI_tau_eta</th>\n",
              "      <th>PRI_tau_phi</th>\n",
              "      <th>PRI_lep_pt</th>\n",
              "      <th>PRI_lep_eta</th>\n",
              "      <th>PRI_lep_phi</th>\n",
              "      <th>PRI_met</th>\n",
              "      <th>PRI_met_phi</th>\n",
              "      <th>PRI_met_sumet</th>\n",
              "      <th>PRI_jet_num</th>\n",
              "      <th>PRI_jet_leading_pt</th>\n",
              "      <th>PRI_jet_leading_eta</th>\n",
              "      <th>PRI_jet_leading_phi</th>\n",
              "      <th>PRI_jet_subleading_pt</th>\n",
              "      <th>PRI_jet_subleading_eta</th>\n",
              "      <th>PRI_jet_subleading_phi</th>\n",
              "      <th>PRI_jet_all_pt</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>138.470</td>\n",
              "      <td>51.655</td>\n",
              "      <td>97.827</td>\n",
              "      <td>27.980</td>\n",
              "      <td>0.91</td>\n",
              "      <td>124.711</td>\n",
              "      <td>2.666</td>\n",
              "      <td>3.064</td>\n",
              "      <td>41.928</td>\n",
              "      <td>197.760</td>\n",
              "      <td>1.582</td>\n",
              "      <td>1.396</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32.638</td>\n",
              "      <td>1.017</td>\n",
              "      <td>0.381</td>\n",
              "      <td>51.626</td>\n",
              "      <td>2.273</td>\n",
              "      <td>-2.414</td>\n",
              "      <td>16.824</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>258.733</td>\n",
              "      <td>2</td>\n",
              "      <td>67.435</td>\n",
              "      <td>2.150</td>\n",
              "      <td>0.444</td>\n",
              "      <td>46.062</td>\n",
              "      <td>1.24</td>\n",
              "      <td>-2.475</td>\n",
              "      <td>113.497</td>\n",
              "      <td>0.002653</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>160.937</td>\n",
              "      <td>68.768</td>\n",
              "      <td>103.235</td>\n",
              "      <td>48.146</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.473</td>\n",
              "      <td>2.078</td>\n",
              "      <td>125.157</td>\n",
              "      <td>0.879</td>\n",
              "      <td>1.414</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>42.014</td>\n",
              "      <td>2.039</td>\n",
              "      <td>-3.011</td>\n",
              "      <td>36.918</td>\n",
              "      <td>0.501</td>\n",
              "      <td>0.103</td>\n",
              "      <td>44.704</td>\n",
              "      <td>-1.916</td>\n",
              "      <td>164.546</td>\n",
              "      <td>1</td>\n",
              "      <td>46.226</td>\n",
              "      <td>0.725</td>\n",
              "      <td>1.158</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>46.226</td>\n",
              "      <td>2.233584</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-999.000</td>\n",
              "      <td>162.172</td>\n",
              "      <td>125.953</td>\n",
              "      <td>35.635</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.148</td>\n",
              "      <td>9.336</td>\n",
              "      <td>197.814</td>\n",
              "      <td>3.776</td>\n",
              "      <td>1.414</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>32.154</td>\n",
              "      <td>-0.705</td>\n",
              "      <td>-2.093</td>\n",
              "      <td>121.409</td>\n",
              "      <td>-0.953</td>\n",
              "      <td>1.052</td>\n",
              "      <td>54.283</td>\n",
              "      <td>-2.186</td>\n",
              "      <td>260.414</td>\n",
              "      <td>1</td>\n",
              "      <td>44.251</td>\n",
              "      <td>2.053</td>\n",
              "      <td>-2.028</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>44.251</td>\n",
              "      <td>2.347389</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>143.905</td>\n",
              "      <td>81.417</td>\n",
              "      <td>80.943</td>\n",
              "      <td>0.414</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.310</td>\n",
              "      <td>0.414</td>\n",
              "      <td>75.968</td>\n",
              "      <td>2.354</td>\n",
              "      <td>-1.285</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>22.647</td>\n",
              "      <td>-1.655</td>\n",
              "      <td>0.010</td>\n",
              "      <td>53.321</td>\n",
              "      <td>-0.522</td>\n",
              "      <td>-3.100</td>\n",
              "      <td>31.082</td>\n",
              "      <td>0.060</td>\n",
              "      <td>86.062</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>5.446378</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>175.864</td>\n",
              "      <td>16.915</td>\n",
              "      <td>134.805</td>\n",
              "      <td>16.405</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.891</td>\n",
              "      <td>16.405</td>\n",
              "      <td>57.983</td>\n",
              "      <td>1.056</td>\n",
              "      <td>-1.385</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>28.209</td>\n",
              "      <td>-2.197</td>\n",
              "      <td>-2.231</td>\n",
              "      <td>29.774</td>\n",
              "      <td>0.798</td>\n",
              "      <td>1.569</td>\n",
              "      <td>2.723</td>\n",
              "      <td>-0.871</td>\n",
              "      <td>53.131</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>6.245333</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   DER_mass_MMC  DER_mass_transverse_met_lep  ...    Weight  Label\n",
              "0       138.470                       51.655  ...  0.002653      1\n",
              "1       160.937                       68.768  ...  2.233584      0\n",
              "2      -999.000                      162.172  ...  2.347389      0\n",
              "3       143.905                       81.417  ...  5.446378      0\n",
              "4       175.864                       16.915  ...  6.245333      0\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcnp3Ix-3qIe"
      },
      "source": [
        "##***2. Data transformation***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5wAPcQd7wcM"
      },
      "source": [
        "###***A. Standardization application***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMM0cz3tHo8w"
      },
      "source": [
        "Due to, after having analyzed the columns in detail, the realizaction that the values had different scales, it was decided to use the standardization technique so that the dataframe could have coherence, and the model, when training, does not get confused because some values they weigh more than others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3W9d5lWGDZ2",
        "outputId": "c5785667-d5f9-4ca5-8836-cd5c30a28f06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "#Standarize\n",
        "dfx = df.iloc[:,:-1]\n",
        "dfx=(dfx-dfx.mean())/dfx.std()\n",
        "dfx.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DER_mass_MMC</th>\n",
              "      <th>DER_mass_transverse_met_lep</th>\n",
              "      <th>DER_mass_vis</th>\n",
              "      <th>DER_pt_h</th>\n",
              "      <th>DER_deltaeta_jet_jet</th>\n",
              "      <th>DER_mass_jet_jet</th>\n",
              "      <th>DER_prodeta_jet_jet</th>\n",
              "      <th>DER_deltar_tau_lep</th>\n",
              "      <th>DER_pt_tot</th>\n",
              "      <th>DER_sum_pt</th>\n",
              "      <th>DER_pt_ratio_lep_tau</th>\n",
              "      <th>DER_met_phi_centrality</th>\n",
              "      <th>DER_lep_eta_centrality</th>\n",
              "      <th>PRI_tau_pt</th>\n",
              "      <th>PRI_tau_eta</th>\n",
              "      <th>PRI_tau_phi</th>\n",
              "      <th>PRI_lep_pt</th>\n",
              "      <th>PRI_lep_eta</th>\n",
              "      <th>PRI_lep_phi</th>\n",
              "      <th>PRI_met</th>\n",
              "      <th>PRI_met_phi</th>\n",
              "      <th>PRI_met_sumet</th>\n",
              "      <th>PRI_jet_num</th>\n",
              "      <th>PRI_jet_leading_pt</th>\n",
              "      <th>PRI_jet_leading_eta</th>\n",
              "      <th>PRI_jet_leading_phi</th>\n",
              "      <th>PRI_jet_subleading_pt</th>\n",
              "      <th>PRI_jet_subleading_eta</th>\n",
              "      <th>PRI_jet_subleading_phi</th>\n",
              "      <th>PRI_jet_all_pt</th>\n",
              "      <th>Weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.461413</td>\n",
              "      <td>0.068332</td>\n",
              "      <td>0.407679</td>\n",
              "      <td>-0.469965</td>\n",
              "      <td>1.560750</td>\n",
              "      <td>1.103311</td>\n",
              "      <td>1.571725</td>\n",
              "      <td>0.882476</td>\n",
              "      <td>1.033096</td>\n",
              "      <td>0.339894</td>\n",
              "      <td>0.170928</td>\n",
              "      <td>1.277081</td>\n",
              "      <td>1.563471</td>\n",
              "      <td>-0.270810</td>\n",
              "      <td>0.846710</td>\n",
              "      <td>0.214211</td>\n",
              "      <td>0.225054</td>\n",
              "      <td>1.812284</td>\n",
              "      <td>-1.352817</td>\n",
              "      <td>-0.756755</td>\n",
              "      <td>-0.147267</td>\n",
              "      <td>0.386846</td>\n",
              "      <td>1.044400</td>\n",
              "      <td>0.780101</td>\n",
              "      <td>0.820300</td>\n",
              "      <td>0.816832</td>\n",
              "      <td>1.538822</td>\n",
              "      <td>1.566797</td>\n",
              "      <td>1.558581</td>\n",
              "      <td>0.412510</td>\n",
              "      <td>-0.876813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.516703</td>\n",
              "      <td>0.552504</td>\n",
              "      <td>0.540135</td>\n",
              "      <td>-0.153167</td>\n",
              "      <td>-0.639366</td>\n",
              "      <td>-0.604528</td>\n",
              "      <td>-0.639361</td>\n",
              "      <td>1.404885</td>\n",
              "      <td>-0.756026</td>\n",
              "      <td>-0.287584</td>\n",
              "      <td>-0.661277</td>\n",
              "      <td>1.292162</td>\n",
              "      <td>-0.639367</td>\n",
              "      <td>0.147536</td>\n",
              "      <td>1.688501</td>\n",
              "      <td>-1.652846</td>\n",
              "      <td>-0.441525</td>\n",
              "      <td>0.411474</td>\n",
              "      <td>0.032730</td>\n",
              "      <td>0.090798</td>\n",
              "      <td>-1.051681</td>\n",
              "      <td>-0.357718</td>\n",
              "      <td>0.021305</td>\n",
              "      <td>0.740306</td>\n",
              "      <td>0.817388</td>\n",
              "      <td>0.818292</td>\n",
              "      <td>-0.638955</td>\n",
              "      <td>-0.639365</td>\n",
              "      <td>-0.639366</td>\n",
              "      <td>-0.273819</td>\n",
              "      <td>0.312952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-2.337854</td>\n",
              "      <td>3.195149</td>\n",
              "      <td>1.096558</td>\n",
              "      <td>-0.349709</td>\n",
              "      <td>-0.639366</td>\n",
              "      <td>-0.604528</td>\n",
              "      <td>-0.639361</td>\n",
              "      <td>0.989768</td>\n",
              "      <td>-0.430167</td>\n",
              "      <td>0.340360</td>\n",
              "      <td>2.768168</td>\n",
              "      <td>1.292162</td>\n",
              "      <td>-0.639367</td>\n",
              "      <td>-0.292406</td>\n",
              "      <td>-0.571649</td>\n",
              "      <td>-1.147551</td>\n",
              "      <td>3.387675</td>\n",
              "      <td>-0.737949</td>\n",
              "      <td>0.555131</td>\n",
              "      <td>0.382000</td>\n",
              "      <td>-1.200670</td>\n",
              "      <td>0.400135</td>\n",
              "      <td>0.021305</td>\n",
              "      <td>0.736600</td>\n",
              "      <td>0.820102</td>\n",
              "      <td>0.811781</td>\n",
              "      <td>-0.638955</td>\n",
              "      <td>-0.639365</td>\n",
              "      <td>-0.639366</td>\n",
              "      <td>-0.293969</td>\n",
              "      <td>0.373644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.474788</td>\n",
              "      <td>0.910377</td>\n",
              "      <td>-0.005853</td>\n",
              "      <td>-0.903014</td>\n",
              "      <td>-0.639366</td>\n",
              "      <td>-0.604528</td>\n",
              "      <td>-0.639361</td>\n",
              "      <td>1.196688</td>\n",
              "      <td>-0.830733</td>\n",
              "      <td>-0.712704</td>\n",
              "      <td>1.084816</td>\n",
              "      <td>-0.969093</td>\n",
              "      <td>-0.639367</td>\n",
              "      <td>-0.716597</td>\n",
              "      <td>-1.354135</td>\n",
              "      <td>0.010002</td>\n",
              "      <td>0.301872</td>\n",
              "      <td>-0.397233</td>\n",
              "      <td>-1.730443</td>\n",
              "      <td>-0.323312</td>\n",
              "      <td>0.038692</td>\n",
              "      <td>-0.978148</td>\n",
              "      <td>-1.001790</td>\n",
              "      <td>-1.220855</td>\n",
              "      <td>-1.225626</td>\n",
              "      <td>-1.225626</td>\n",
              "      <td>-0.638955</td>\n",
              "      <td>-0.639365</td>\n",
              "      <td>-0.639366</td>\n",
              "      <td>-0.745438</td>\n",
              "      <td>2.026347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.553438</td>\n",
              "      <td>-0.914554</td>\n",
              "      <td>1.313366</td>\n",
              "      <td>-0.651803</td>\n",
              "      <td>-0.639366</td>\n",
              "      <td>-0.604528</td>\n",
              "      <td>-0.639361</td>\n",
              "      <td>1.938790</td>\n",
              "      <td>-0.112795</td>\n",
              "      <td>-0.868141</td>\n",
              "      <td>-0.451746</td>\n",
              "      <td>-1.052875</td>\n",
              "      <td>-0.639367</td>\n",
              "      <td>-0.468427</td>\n",
              "      <td>-1.800565</td>\n",
              "      <td>-1.223511</td>\n",
              "      <td>-0.765296</td>\n",
              "      <td>0.646260</td>\n",
              "      <td>0.839727</td>\n",
              "      <td>-1.185426</td>\n",
              "      <td>-0.475041</td>\n",
              "      <td>-1.238473</td>\n",
              "      <td>-1.001790</td>\n",
              "      <td>-1.220855</td>\n",
              "      <td>-1.225626</td>\n",
              "      <td>-1.225626</td>\n",
              "      <td>-0.638955</td>\n",
              "      <td>-0.639365</td>\n",
              "      <td>-0.639366</td>\n",
              "      <td>-0.745438</td>\n",
              "      <td>2.452433</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   DER_mass_MMC  DER_mass_transverse_met_lep  ...  PRI_jet_all_pt    Weight\n",
              "0      0.461413                     0.068332  ...        0.412510 -0.876813\n",
              "1      0.516703                     0.552504  ...       -0.273819  0.312952\n",
              "2     -2.337854                     3.195149  ...       -0.293969  0.373644\n",
              "3      0.474788                     0.910377  ...       -0.745438  2.026347\n",
              "4      0.553438                    -0.914554  ...       -0.745438  2.452433\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pZFY7ESgaYc"
      },
      "source": [
        "##***3. Feature Selection***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_4dhRY2K8Ac"
      },
      "source": [
        "###***A. Principal Component Analysis (PCA) application***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63V5OcDGjOYu"
      },
      "source": [
        "It was also possible to analyze that the dataset contained several columns, that is, it had a high dimension. To deal with this, it was decided to apply Principle Component Analysis (PCA). This reduces the dataset to fewer columns so that the model is not too complicated to train. In this case, we consider to get 15 principle components. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yv6hMQZLnyk",
        "outputId": "3b7e7ad1-4a98-4063-f4e7-d640912d5d92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=15)\n",
        "pcomp = pca.fit_transform(dfx)\n",
        "\n",
        "pcadf = pd.DataFrame(data = pcomp)\n",
        "pcadf.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.909106</td>\n",
              "      <td>0.578580</td>\n",
              "      <td>-1.807047</td>\n",
              "      <td>-1.797587</td>\n",
              "      <td>-0.675406</td>\n",
              "      <td>1.755921</td>\n",
              "      <td>1.133940</td>\n",
              "      <td>-0.102496</td>\n",
              "      <td>-0.364727</td>\n",
              "      <td>1.106631</td>\n",
              "      <td>-0.720947</td>\n",
              "      <td>-0.239374</td>\n",
              "      <td>0.683197</td>\n",
              "      <td>-0.667598</td>\n",
              "      <td>0.761884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.002433</td>\n",
              "      <td>-1.702615</td>\n",
              "      <td>0.645995</td>\n",
              "      <td>-1.287503</td>\n",
              "      <td>-0.579928</td>\n",
              "      <td>1.387507</td>\n",
              "      <td>-1.224460</td>\n",
              "      <td>-1.289892</td>\n",
              "      <td>-1.152271</td>\n",
              "      <td>0.111834</td>\n",
              "      <td>-0.957205</td>\n",
              "      <td>1.354163</td>\n",
              "      <td>0.002844</td>\n",
              "      <td>-0.684504</td>\n",
              "      <td>-0.800538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.727750</td>\n",
              "      <td>2.635592</td>\n",
              "      <td>4.314479</td>\n",
              "      <td>-1.770568</td>\n",
              "      <td>-1.889024</td>\n",
              "      <td>-1.136834</td>\n",
              "      <td>-1.324711</td>\n",
              "      <td>-1.528420</td>\n",
              "      <td>-1.087185</td>\n",
              "      <td>-0.232950</td>\n",
              "      <td>-0.230793</td>\n",
              "      <td>0.020733</td>\n",
              "      <td>1.886853</td>\n",
              "      <td>-1.489323</td>\n",
              "      <td>0.051157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-3.763644</td>\n",
              "      <td>1.936254</td>\n",
              "      <td>-0.095562</td>\n",
              "      <td>-0.815837</td>\n",
              "      <td>-0.026251</td>\n",
              "      <td>-1.256957</td>\n",
              "      <td>1.232349</td>\n",
              "      <td>0.575441</td>\n",
              "      <td>-0.223774</td>\n",
              "      <td>-0.473996</td>\n",
              "      <td>-1.202835</td>\n",
              "      <td>0.573624</td>\n",
              "      <td>-0.948401</td>\n",
              "      <td>0.155328</td>\n",
              "      <td>0.704654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-3.963918</td>\n",
              "      <td>0.526049</td>\n",
              "      <td>-1.394823</td>\n",
              "      <td>-1.491979</td>\n",
              "      <td>1.316625</td>\n",
              "      <td>-0.677281</td>\n",
              "      <td>-1.432062</td>\n",
              "      <td>0.235733</td>\n",
              "      <td>-0.468412</td>\n",
              "      <td>0.348293</td>\n",
              "      <td>-0.173563</td>\n",
              "      <td>1.061484</td>\n",
              "      <td>-1.878576</td>\n",
              "      <td>0.025525</td>\n",
              "      <td>1.805419</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...        12        13        14\n",
              "0  3.909106  0.578580 -1.807047  ...  0.683197 -0.667598  0.761884\n",
              "1 -1.002433 -1.702615  0.645995  ...  0.002844 -0.684504 -0.800538\n",
              "2 -0.727750  2.635592  4.314479  ...  1.886853 -1.489323  0.051157\n",
              "3 -3.763644  1.936254 -0.095562  ... -0.948401  0.155328  0.704654\n",
              "4 -3.963918  0.526049 -1.394823  ... -1.878576  0.025525  1.805419\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmVkXWghMN5R"
      },
      "source": [
        "In the following code, we are adding the class column ***Label*** in the dataset where we apply the feature selection. In that way, we can have the principal components obtained and the class column ***Label*** in one dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNv9JV8OGklV",
        "outputId": "23c97cc5-0ed1-4e38-9c3c-1d87755660ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "ofidf = pcadf\n",
        "ofidf[\"Label\"] = df[\"Label\"].values\n",
        "ofidf.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.909106</td>\n",
              "      <td>0.578580</td>\n",
              "      <td>-1.807047</td>\n",
              "      <td>-1.797587</td>\n",
              "      <td>-0.675406</td>\n",
              "      <td>1.755921</td>\n",
              "      <td>1.133940</td>\n",
              "      <td>-0.102496</td>\n",
              "      <td>-0.364727</td>\n",
              "      <td>1.106631</td>\n",
              "      <td>-0.720947</td>\n",
              "      <td>-0.239374</td>\n",
              "      <td>0.683197</td>\n",
              "      <td>-0.667598</td>\n",
              "      <td>0.761884</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.002433</td>\n",
              "      <td>-1.702615</td>\n",
              "      <td>0.645995</td>\n",
              "      <td>-1.287503</td>\n",
              "      <td>-0.579928</td>\n",
              "      <td>1.387507</td>\n",
              "      <td>-1.224460</td>\n",
              "      <td>-1.289892</td>\n",
              "      <td>-1.152271</td>\n",
              "      <td>0.111834</td>\n",
              "      <td>-0.957205</td>\n",
              "      <td>1.354163</td>\n",
              "      <td>0.002844</td>\n",
              "      <td>-0.684504</td>\n",
              "      <td>-0.800538</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.727750</td>\n",
              "      <td>2.635592</td>\n",
              "      <td>4.314479</td>\n",
              "      <td>-1.770568</td>\n",
              "      <td>-1.889024</td>\n",
              "      <td>-1.136834</td>\n",
              "      <td>-1.324711</td>\n",
              "      <td>-1.528420</td>\n",
              "      <td>-1.087185</td>\n",
              "      <td>-0.232950</td>\n",
              "      <td>-0.230793</td>\n",
              "      <td>0.020733</td>\n",
              "      <td>1.886853</td>\n",
              "      <td>-1.489323</td>\n",
              "      <td>0.051157</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-3.763644</td>\n",
              "      <td>1.936254</td>\n",
              "      <td>-0.095562</td>\n",
              "      <td>-0.815837</td>\n",
              "      <td>-0.026251</td>\n",
              "      <td>-1.256957</td>\n",
              "      <td>1.232349</td>\n",
              "      <td>0.575441</td>\n",
              "      <td>-0.223774</td>\n",
              "      <td>-0.473996</td>\n",
              "      <td>-1.202835</td>\n",
              "      <td>0.573624</td>\n",
              "      <td>-0.948401</td>\n",
              "      <td>0.155328</td>\n",
              "      <td>0.704654</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-3.963918</td>\n",
              "      <td>0.526049</td>\n",
              "      <td>-1.394823</td>\n",
              "      <td>-1.491979</td>\n",
              "      <td>1.316625</td>\n",
              "      <td>-0.677281</td>\n",
              "      <td>-1.432062</td>\n",
              "      <td>0.235733</td>\n",
              "      <td>-0.468412</td>\n",
              "      <td>0.348293</td>\n",
              "      <td>-0.173563</td>\n",
              "      <td>1.061484</td>\n",
              "      <td>-1.878576</td>\n",
              "      <td>0.025525</td>\n",
              "      <td>1.805419</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3  ...        12        13        14  Label\n",
              "0  3.909106  0.578580 -1.807047 -1.797587  ...  0.683197 -0.667598  0.761884      1\n",
              "1 -1.002433 -1.702615  0.645995 -1.287503  ...  0.002844 -0.684504 -0.800538      0\n",
              "2 -0.727750  2.635592  4.314479 -1.770568  ...  1.886853 -1.489323  0.051157      0\n",
              "3 -3.763644  1.936254 -0.095562 -0.815837  ... -0.948401  0.155328  0.704654      0\n",
              "4 -3.963918  0.526049 -1.394823 -1.491979  ... -1.878576  0.025525  1.805419      0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmvqTaJxN47j"
      },
      "source": [
        "#***III. Test with a Deep Learning model***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmWG4FJjSrcx"
      },
      "source": [
        "First of all, the columns for training were separated from the class columns, to be able to pass them to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdIUdFe-3-Gj"
      },
      "source": [
        "X = ofidf.iloc[:,0:-1]\n",
        "y = ofidf.iloc[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6lpDwhyOfyg"
      },
      "source": [
        "In this case, we going to import ***keras*** from ***TensorFlow*** to use a ***Deep Learning*** model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChqYuyzp3LNn"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb5ISrP2PPAV"
      },
      "source": [
        "##***1. Technique application***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbZlHeYYO7Le"
      },
      "source": [
        "***Approach:*** In this part a Deep Learning model is developed. Regarding its architecture, this model contains 5 layers. First, it must be defined why the application of models of this type in learning problems is important. Deep Learning models are basically neural networks that are characterized by containing a large number of layers or neurons. For this reason, they are considered deep learning models. The importance of its use is based on the great ability of this type of algorithm to offer results comparable to those of a human expert. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSBp_Jc0TJXA"
      },
      "source": [
        "In the following code, the described model is implemented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD42Z8043NN4"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(32, input_dim=15, activation='linear'))\n",
        "model.add(Dense(16, activation='linear'))\n",
        "model.add(Dense(16, activation='tanh'))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "sgd = SGD(lr=1/30)\n",
        "model.compile(optimizer=\"adagrad\", loss='mse', metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh1_N5qBWlDt"
      },
      "source": [
        "Now, the model is trained according to the code specified in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bg5FQC83T68",
        "outputId": "34ed380c-e913-4f18-bb2d-f0f282ca31e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = model.fit(X, y, validation_split=0.3, epochs=500, verbose=1, batch_size=512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.2094 - accuracy: 0.6905 - val_loss: 0.1934 - val_accuracy: 0.7180\n",
            "Epoch 2/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.1818 - accuracy: 0.7395 - val_loss: 0.1717 - val_accuracy: 0.7567\n",
            "Epoch 3/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.1622 - accuracy: 0.7747 - val_loss: 0.1537 - val_accuracy: 0.7901\n",
            "Epoch 4/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.1454 - accuracy: 0.8061 - val_loss: 0.1382 - val_accuracy: 0.8196\n",
            "Epoch 5/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.1309 - accuracy: 0.8337 - val_loss: 0.1248 - val_accuracy: 0.8445\n",
            "Epoch 6/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.1184 - accuracy: 0.8564 - val_loss: 0.1133 - val_accuracy: 0.8653\n",
            "Epoch 7/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.1077 - accuracy: 0.8754 - val_loss: 0.1036 - val_accuracy: 0.8817\n",
            "Epoch 8/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0987 - accuracy: 0.8891 - val_loss: 0.0954 - val_accuracy: 0.8937\n",
            "Epoch 9/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0912 - accuracy: 0.8992 - val_loss: 0.0884 - val_accuracy: 0.9006\n",
            "Epoch 10/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0848 - accuracy: 0.9052 - val_loss: 0.0826 - val_accuracy: 0.9061\n",
            "Epoch 11/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0795 - accuracy: 0.9096 - val_loss: 0.0778 - val_accuracy: 0.9097\n",
            "Epoch 12/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0751 - accuracy: 0.9128 - val_loss: 0.0738 - val_accuracy: 0.9122\n",
            "Epoch 13/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0715 - accuracy: 0.9153 - val_loss: 0.0705 - val_accuracy: 0.9144\n",
            "Epoch 14/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0685 - accuracy: 0.9173 - val_loss: 0.0678 - val_accuracy: 0.9167\n",
            "Epoch 15/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0660 - accuracy: 0.9190 - val_loss: 0.0655 - val_accuracy: 0.9186\n",
            "Epoch 16/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0640 - accuracy: 0.9203 - val_loss: 0.0636 - val_accuracy: 0.9197\n",
            "Epoch 17/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0622 - accuracy: 0.9215 - val_loss: 0.0620 - val_accuracy: 0.9217\n",
            "Epoch 18/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0607 - accuracy: 0.9230 - val_loss: 0.0606 - val_accuracy: 0.9227\n",
            "Epoch 19/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0594 - accuracy: 0.9237 - val_loss: 0.0594 - val_accuracy: 0.9237\n",
            "Epoch 20/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0583 - accuracy: 0.9249 - val_loss: 0.0584 - val_accuracy: 0.9243\n",
            "Epoch 21/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0573 - accuracy: 0.9257 - val_loss: 0.0574 - val_accuracy: 0.9254\n",
            "Epoch 22/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0564 - accuracy: 0.9265 - val_loss: 0.0566 - val_accuracy: 0.9261\n",
            "Epoch 23/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0556 - accuracy: 0.9273 - val_loss: 0.0558 - val_accuracy: 0.9267\n",
            "Epoch 24/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0549 - accuracy: 0.9279 - val_loss: 0.0552 - val_accuracy: 0.9274\n",
            "Epoch 25/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0542 - accuracy: 0.9285 - val_loss: 0.0546 - val_accuracy: 0.9286\n",
            "Epoch 26/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0536 - accuracy: 0.9292 - val_loss: 0.0540 - val_accuracy: 0.9288\n",
            "Epoch 27/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0531 - accuracy: 0.9298 - val_loss: 0.0534 - val_accuracy: 0.9293\n",
            "Epoch 28/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0526 - accuracy: 0.9303 - val_loss: 0.0529 - val_accuracy: 0.9299\n",
            "Epoch 29/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0521 - accuracy: 0.9310 - val_loss: 0.0525 - val_accuracy: 0.9305\n",
            "Epoch 30/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0517 - accuracy: 0.9315 - val_loss: 0.0521 - val_accuracy: 0.9309\n",
            "Epoch 31/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0513 - accuracy: 0.9320 - val_loss: 0.0516 - val_accuracy: 0.9317\n",
            "Epoch 32/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0509 - accuracy: 0.9326 - val_loss: 0.0513 - val_accuracy: 0.9323\n",
            "Epoch 33/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0505 - accuracy: 0.9330 - val_loss: 0.0509 - val_accuracy: 0.9323\n",
            "Epoch 34/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0502 - accuracy: 0.9335 - val_loss: 0.0506 - val_accuracy: 0.9329\n",
            "Epoch 35/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0498 - accuracy: 0.9338 - val_loss: 0.0503 - val_accuracy: 0.9331\n",
            "Epoch 36/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0495 - accuracy: 0.9342 - val_loss: 0.0499 - val_accuracy: 0.9335\n",
            "Epoch 37/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0492 - accuracy: 0.9347 - val_loss: 0.0497 - val_accuracy: 0.9340\n",
            "Epoch 38/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0489 - accuracy: 0.9350 - val_loss: 0.0494 - val_accuracy: 0.9344\n",
            "Epoch 39/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0486 - accuracy: 0.9354 - val_loss: 0.0491 - val_accuracy: 0.9348\n",
            "Epoch 40/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0484 - accuracy: 0.9358 - val_loss: 0.0489 - val_accuracy: 0.9350\n",
            "Epoch 41/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0481 - accuracy: 0.9361 - val_loss: 0.0486 - val_accuracy: 0.9356\n",
            "Epoch 42/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0479 - accuracy: 0.9366 - val_loss: 0.0484 - val_accuracy: 0.9356\n",
            "Epoch 43/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0477 - accuracy: 0.9367 - val_loss: 0.0481 - val_accuracy: 0.9361\n",
            "Epoch 44/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0474 - accuracy: 0.9372 - val_loss: 0.0479 - val_accuracy: 0.9366\n",
            "Epoch 45/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0472 - accuracy: 0.9375 - val_loss: 0.0477 - val_accuracy: 0.9368\n",
            "Epoch 46/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0470 - accuracy: 0.9377 - val_loss: 0.0475 - val_accuracy: 0.9373\n",
            "Epoch 47/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0468 - accuracy: 0.9381 - val_loss: 0.0473 - val_accuracy: 0.9378\n",
            "Epoch 48/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0466 - accuracy: 0.9383 - val_loss: 0.0471 - val_accuracy: 0.9379\n",
            "Epoch 49/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0464 - accuracy: 0.9386 - val_loss: 0.0469 - val_accuracy: 0.9381\n",
            "Epoch 50/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0462 - accuracy: 0.9388 - val_loss: 0.0468 - val_accuracy: 0.9386\n",
            "Epoch 51/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0461 - accuracy: 0.9391 - val_loss: 0.0466 - val_accuracy: 0.9385\n",
            "Epoch 52/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0459 - accuracy: 0.9394 - val_loss: 0.0464 - val_accuracy: 0.9387\n",
            "Epoch 53/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0457 - accuracy: 0.9395 - val_loss: 0.0463 - val_accuracy: 0.9389\n",
            "Epoch 54/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0456 - accuracy: 0.9399 - val_loss: 0.0461 - val_accuracy: 0.9393\n",
            "Epoch 55/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0454 - accuracy: 0.9399 - val_loss: 0.0460 - val_accuracy: 0.9392\n",
            "Epoch 56/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0453 - accuracy: 0.9403 - val_loss: 0.0458 - val_accuracy: 0.9398\n",
            "Epoch 57/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0451 - accuracy: 0.9405 - val_loss: 0.0457 - val_accuracy: 0.9398\n",
            "Epoch 58/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0450 - accuracy: 0.9407 - val_loss: 0.0455 - val_accuracy: 0.9401\n",
            "Epoch 59/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0448 - accuracy: 0.9407 - val_loss: 0.0454 - val_accuracy: 0.9405\n",
            "Epoch 60/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0447 - accuracy: 0.9410 - val_loss: 0.0453 - val_accuracy: 0.9407\n",
            "Epoch 61/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0446 - accuracy: 0.9412 - val_loss: 0.0451 - val_accuracy: 0.9407\n",
            "Epoch 62/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0445 - accuracy: 0.9414 - val_loss: 0.0450 - val_accuracy: 0.9408\n",
            "Epoch 63/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0443 - accuracy: 0.9415 - val_loss: 0.0449 - val_accuracy: 0.9408\n",
            "Epoch 64/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0442 - accuracy: 0.9416 - val_loss: 0.0448 - val_accuracy: 0.9411\n",
            "Epoch 65/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0441 - accuracy: 0.9417 - val_loss: 0.0447 - val_accuracy: 0.9412\n",
            "Epoch 66/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0440 - accuracy: 0.9420 - val_loss: 0.0446 - val_accuracy: 0.9414\n",
            "Epoch 67/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0439 - accuracy: 0.9420 - val_loss: 0.0445 - val_accuracy: 0.9415\n",
            "Epoch 68/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0438 - accuracy: 0.9422 - val_loss: 0.0443 - val_accuracy: 0.9416\n",
            "Epoch 69/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0437 - accuracy: 0.9424 - val_loss: 0.0443 - val_accuracy: 0.9416\n",
            "Epoch 70/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0436 - accuracy: 0.9425 - val_loss: 0.0442 - val_accuracy: 0.9420\n",
            "Epoch 71/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0435 - accuracy: 0.9426 - val_loss: 0.0441 - val_accuracy: 0.9420\n",
            "Epoch 72/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0434 - accuracy: 0.9429 - val_loss: 0.0440 - val_accuracy: 0.9419\n",
            "Epoch 73/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0433 - accuracy: 0.9429 - val_loss: 0.0439 - val_accuracy: 0.9424\n",
            "Epoch 74/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0432 - accuracy: 0.9431 - val_loss: 0.0438 - val_accuracy: 0.9423\n",
            "Epoch 75/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0431 - accuracy: 0.9432 - val_loss: 0.0437 - val_accuracy: 0.9425\n",
            "Epoch 76/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0430 - accuracy: 0.9434 - val_loss: 0.0436 - val_accuracy: 0.9426\n",
            "Epoch 77/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0429 - accuracy: 0.9433 - val_loss: 0.0435 - val_accuracy: 0.9427\n",
            "Epoch 78/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0429 - accuracy: 0.9435 - val_loss: 0.0435 - val_accuracy: 0.9427\n",
            "Epoch 79/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0428 - accuracy: 0.9436 - val_loss: 0.0434 - val_accuracy: 0.9429\n",
            "Epoch 80/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0427 - accuracy: 0.9437 - val_loss: 0.0433 - val_accuracy: 0.9430\n",
            "Epoch 81/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0426 - accuracy: 0.9438 - val_loss: 0.0432 - val_accuracy: 0.9427\n",
            "Epoch 82/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0425 - accuracy: 0.9439 - val_loss: 0.0432 - val_accuracy: 0.9430\n",
            "Epoch 83/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0425 - accuracy: 0.9441 - val_loss: 0.0431 - val_accuracy: 0.9428\n",
            "Epoch 84/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0424 - accuracy: 0.9441 - val_loss: 0.0430 - val_accuracy: 0.9428\n",
            "Epoch 85/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0423 - accuracy: 0.9441 - val_loss: 0.0429 - val_accuracy: 0.9432\n",
            "Epoch 86/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0423 - accuracy: 0.9443 - val_loss: 0.0429 - val_accuracy: 0.9433\n",
            "Epoch 87/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0422 - accuracy: 0.9444 - val_loss: 0.0428 - val_accuracy: 0.9435\n",
            "Epoch 88/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0421 - accuracy: 0.9444 - val_loss: 0.0428 - val_accuracy: 0.9431\n",
            "Epoch 89/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0421 - accuracy: 0.9444 - val_loss: 0.0427 - val_accuracy: 0.9434\n",
            "Epoch 90/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0420 - accuracy: 0.9446 - val_loss: 0.0426 - val_accuracy: 0.9436\n",
            "Epoch 91/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0419 - accuracy: 0.9447 - val_loss: 0.0425 - val_accuracy: 0.9437\n",
            "Epoch 92/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0419 - accuracy: 0.9448 - val_loss: 0.0425 - val_accuracy: 0.9437\n",
            "Epoch 93/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0418 - accuracy: 0.9448 - val_loss: 0.0424 - val_accuracy: 0.9438\n",
            "Epoch 94/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0417 - accuracy: 0.9449 - val_loss: 0.0424 - val_accuracy: 0.9438\n",
            "Epoch 95/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0417 - accuracy: 0.9449 - val_loss: 0.0423 - val_accuracy: 0.9438\n",
            "Epoch 96/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0416 - accuracy: 0.9450 - val_loss: 0.0423 - val_accuracy: 0.9440\n",
            "Epoch 97/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0416 - accuracy: 0.9451 - val_loss: 0.0422 - val_accuracy: 0.9440\n",
            "Epoch 98/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0415 - accuracy: 0.9451 - val_loss: 0.0421 - val_accuracy: 0.9441\n",
            "Epoch 99/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0414 - accuracy: 0.9452 - val_loss: 0.0421 - val_accuracy: 0.9441\n",
            "Epoch 100/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0414 - accuracy: 0.9452 - val_loss: 0.0420 - val_accuracy: 0.9443\n",
            "Epoch 101/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0413 - accuracy: 0.9452 - val_loss: 0.0420 - val_accuracy: 0.9443\n",
            "Epoch 102/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0413 - accuracy: 0.9454 - val_loss: 0.0419 - val_accuracy: 0.9443\n",
            "Epoch 103/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0412 - accuracy: 0.9454 - val_loss: 0.0419 - val_accuracy: 0.9444\n",
            "Epoch 104/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0412 - accuracy: 0.9455 - val_loss: 0.0418 - val_accuracy: 0.9444\n",
            "Epoch 105/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0411 - accuracy: 0.9455 - val_loss: 0.0418 - val_accuracy: 0.9447\n",
            "Epoch 106/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0411 - accuracy: 0.9456 - val_loss: 0.0417 - val_accuracy: 0.9446\n",
            "Epoch 107/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0410 - accuracy: 0.9457 - val_loss: 0.0417 - val_accuracy: 0.9446\n",
            "Epoch 108/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0410 - accuracy: 0.9457 - val_loss: 0.0416 - val_accuracy: 0.9448\n",
            "Epoch 109/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0409 - accuracy: 0.9459 - val_loss: 0.0416 - val_accuracy: 0.9448\n",
            "Epoch 110/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0409 - accuracy: 0.9458 - val_loss: 0.0415 - val_accuracy: 0.9448\n",
            "Epoch 111/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0408 - accuracy: 0.9459 - val_loss: 0.0415 - val_accuracy: 0.9450\n",
            "Epoch 112/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0408 - accuracy: 0.9461 - val_loss: 0.0414 - val_accuracy: 0.9451\n",
            "Epoch 113/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0407 - accuracy: 0.9462 - val_loss: 0.0414 - val_accuracy: 0.9450\n",
            "Epoch 114/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0407 - accuracy: 0.9461 - val_loss: 0.0413 - val_accuracy: 0.9451\n",
            "Epoch 115/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0406 - accuracy: 0.9462 - val_loss: 0.0413 - val_accuracy: 0.9451\n",
            "Epoch 116/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0406 - accuracy: 0.9464 - val_loss: 0.0412 - val_accuracy: 0.9453\n",
            "Epoch 117/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0405 - accuracy: 0.9463 - val_loss: 0.0412 - val_accuracy: 0.9452\n",
            "Epoch 118/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0405 - accuracy: 0.9464 - val_loss: 0.0412 - val_accuracy: 0.9453\n",
            "Epoch 119/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0404 - accuracy: 0.9465 - val_loss: 0.0411 - val_accuracy: 0.9454\n",
            "Epoch 120/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0404 - accuracy: 0.9465 - val_loss: 0.0411 - val_accuracy: 0.9454\n",
            "Epoch 121/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0403 - accuracy: 0.9466 - val_loss: 0.0410 - val_accuracy: 0.9454\n",
            "Epoch 122/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0403 - accuracy: 0.9465 - val_loss: 0.0410 - val_accuracy: 0.9455\n",
            "Epoch 123/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0402 - accuracy: 0.9467 - val_loss: 0.0409 - val_accuracy: 0.9456\n",
            "Epoch 124/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0402 - accuracy: 0.9468 - val_loss: 0.0409 - val_accuracy: 0.9456\n",
            "Epoch 125/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0401 - accuracy: 0.9469 - val_loss: 0.0409 - val_accuracy: 0.9455\n",
            "Epoch 126/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0401 - accuracy: 0.9469 - val_loss: 0.0408 - val_accuracy: 0.9456\n",
            "Epoch 127/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0401 - accuracy: 0.9470 - val_loss: 0.0408 - val_accuracy: 0.9457\n",
            "Epoch 128/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0400 - accuracy: 0.9471 - val_loss: 0.0407 - val_accuracy: 0.9457\n",
            "Epoch 129/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0400 - accuracy: 0.9471 - val_loss: 0.0407 - val_accuracy: 0.9459\n",
            "Epoch 130/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0399 - accuracy: 0.9473 - val_loss: 0.0406 - val_accuracy: 0.9459\n",
            "Epoch 131/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0399 - accuracy: 0.9473 - val_loss: 0.0406 - val_accuracy: 0.9460\n",
            "Epoch 132/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0398 - accuracy: 0.9473 - val_loss: 0.0406 - val_accuracy: 0.9460\n",
            "Epoch 133/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0398 - accuracy: 0.9474 - val_loss: 0.0405 - val_accuracy: 0.9461\n",
            "Epoch 134/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0397 - accuracy: 0.9474 - val_loss: 0.0405 - val_accuracy: 0.9462\n",
            "Epoch 135/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0397 - accuracy: 0.9476 - val_loss: 0.0404 - val_accuracy: 0.9463\n",
            "Epoch 136/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0397 - accuracy: 0.9475 - val_loss: 0.0404 - val_accuracy: 0.9464\n",
            "Epoch 137/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0396 - accuracy: 0.9476 - val_loss: 0.0403 - val_accuracy: 0.9464\n",
            "Epoch 138/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0396 - accuracy: 0.9476 - val_loss: 0.0403 - val_accuracy: 0.9465\n",
            "Epoch 139/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0395 - accuracy: 0.9476 - val_loss: 0.0403 - val_accuracy: 0.9465\n",
            "Epoch 140/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0395 - accuracy: 0.9477 - val_loss: 0.0402 - val_accuracy: 0.9465\n",
            "Epoch 141/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0395 - accuracy: 0.9477 - val_loss: 0.0402 - val_accuracy: 0.9466\n",
            "Epoch 142/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0394 - accuracy: 0.9478 - val_loss: 0.0401 - val_accuracy: 0.9466\n",
            "Epoch 143/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0394 - accuracy: 0.9478 - val_loss: 0.0401 - val_accuracy: 0.9466\n",
            "Epoch 144/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0393 - accuracy: 0.9479 - val_loss: 0.0401 - val_accuracy: 0.9467\n",
            "Epoch 145/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0393 - accuracy: 0.9479 - val_loss: 0.0400 - val_accuracy: 0.9468\n",
            "Epoch 146/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0392 - accuracy: 0.9480 - val_loss: 0.0400 - val_accuracy: 0.9469\n",
            "Epoch 147/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0392 - accuracy: 0.9480 - val_loss: 0.0399 - val_accuracy: 0.9469\n",
            "Epoch 148/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0392 - accuracy: 0.9481 - val_loss: 0.0399 - val_accuracy: 0.9469\n",
            "Epoch 149/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0391 - accuracy: 0.9482 - val_loss: 0.0399 - val_accuracy: 0.9469\n",
            "Epoch 150/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0391 - accuracy: 0.9481 - val_loss: 0.0398 - val_accuracy: 0.9469\n",
            "Epoch 151/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0390 - accuracy: 0.9483 - val_loss: 0.0398 - val_accuracy: 0.9469\n",
            "Epoch 152/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0390 - accuracy: 0.9483 - val_loss: 0.0397 - val_accuracy: 0.9469\n",
            "Epoch 153/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0390 - accuracy: 0.9483 - val_loss: 0.0397 - val_accuracy: 0.9471\n",
            "Epoch 154/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0389 - accuracy: 0.9484 - val_loss: 0.0397 - val_accuracy: 0.9470\n",
            "Epoch 155/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0389 - accuracy: 0.9485 - val_loss: 0.0396 - val_accuracy: 0.9470\n",
            "Epoch 156/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0388 - accuracy: 0.9485 - val_loss: 0.0396 - val_accuracy: 0.9472\n",
            "Epoch 157/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0388 - accuracy: 0.9485 - val_loss: 0.0395 - val_accuracy: 0.9474\n",
            "Epoch 158/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0388 - accuracy: 0.9487 - val_loss: 0.0395 - val_accuracy: 0.9472\n",
            "Epoch 159/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0387 - accuracy: 0.9486 - val_loss: 0.0395 - val_accuracy: 0.9474\n",
            "Epoch 160/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0387 - accuracy: 0.9486 - val_loss: 0.0394 - val_accuracy: 0.9474\n",
            "Epoch 161/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0387 - accuracy: 0.9486 - val_loss: 0.0394 - val_accuracy: 0.9475\n",
            "Epoch 162/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0386 - accuracy: 0.9488 - val_loss: 0.0393 - val_accuracy: 0.9475\n",
            "Epoch 163/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0386 - accuracy: 0.9489 - val_loss: 0.0393 - val_accuracy: 0.9476\n",
            "Epoch 164/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0385 - accuracy: 0.9489 - val_loss: 0.0393 - val_accuracy: 0.9476\n",
            "Epoch 165/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0385 - accuracy: 0.9490 - val_loss: 0.0392 - val_accuracy: 0.9477\n",
            "Epoch 166/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0385 - accuracy: 0.9490 - val_loss: 0.0392 - val_accuracy: 0.9476\n",
            "Epoch 167/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0384 - accuracy: 0.9491 - val_loss: 0.0392 - val_accuracy: 0.9477\n",
            "Epoch 168/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0384 - accuracy: 0.9492 - val_loss: 0.0391 - val_accuracy: 0.9477\n",
            "Epoch 169/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0384 - accuracy: 0.9492 - val_loss: 0.0391 - val_accuracy: 0.9478\n",
            "Epoch 170/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0383 - accuracy: 0.9493 - val_loss: 0.0390 - val_accuracy: 0.9477\n",
            "Epoch 171/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0383 - accuracy: 0.9494 - val_loss: 0.0390 - val_accuracy: 0.9478\n",
            "Epoch 172/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0382 - accuracy: 0.9494 - val_loss: 0.0390 - val_accuracy: 0.9478\n",
            "Epoch 173/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0382 - accuracy: 0.9495 - val_loss: 0.0389 - val_accuracy: 0.9479\n",
            "Epoch 174/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0382 - accuracy: 0.9496 - val_loss: 0.0389 - val_accuracy: 0.9479\n",
            "Epoch 175/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0381 - accuracy: 0.9496 - val_loss: 0.0389 - val_accuracy: 0.9479\n",
            "Epoch 176/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0381 - accuracy: 0.9496 - val_loss: 0.0388 - val_accuracy: 0.9481\n",
            "Epoch 177/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0381 - accuracy: 0.9497 - val_loss: 0.0388 - val_accuracy: 0.9480\n",
            "Epoch 178/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0380 - accuracy: 0.9498 - val_loss: 0.0388 - val_accuracy: 0.9482\n",
            "Epoch 179/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0380 - accuracy: 0.9497 - val_loss: 0.0387 - val_accuracy: 0.9481\n",
            "Epoch 180/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0380 - accuracy: 0.9498 - val_loss: 0.0387 - val_accuracy: 0.9483\n",
            "Epoch 181/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0379 - accuracy: 0.9498 - val_loss: 0.0386 - val_accuracy: 0.9483\n",
            "Epoch 182/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0379 - accuracy: 0.9498 - val_loss: 0.0386 - val_accuracy: 0.9484\n",
            "Epoch 183/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0378 - accuracy: 0.9500 - val_loss: 0.0386 - val_accuracy: 0.9484\n",
            "Epoch 184/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0378 - accuracy: 0.9499 - val_loss: 0.0385 - val_accuracy: 0.9483\n",
            "Epoch 185/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0378 - accuracy: 0.9500 - val_loss: 0.0385 - val_accuracy: 0.9483\n",
            "Epoch 186/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0377 - accuracy: 0.9500 - val_loss: 0.0385 - val_accuracy: 0.9485\n",
            "Epoch 187/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0377 - accuracy: 0.9501 - val_loss: 0.0384 - val_accuracy: 0.9485\n",
            "Epoch 188/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0377 - accuracy: 0.9502 - val_loss: 0.0384 - val_accuracy: 0.9485\n",
            "Epoch 189/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0376 - accuracy: 0.9502 - val_loss: 0.0384 - val_accuracy: 0.9487\n",
            "Epoch 190/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0376 - accuracy: 0.9502 - val_loss: 0.0383 - val_accuracy: 0.9487\n",
            "Epoch 191/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0376 - accuracy: 0.9503 - val_loss: 0.0383 - val_accuracy: 0.9487\n",
            "Epoch 192/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0375 - accuracy: 0.9503 - val_loss: 0.0383 - val_accuracy: 0.9489\n",
            "Epoch 193/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0375 - accuracy: 0.9503 - val_loss: 0.0382 - val_accuracy: 0.9488\n",
            "Epoch 194/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0375 - accuracy: 0.9504 - val_loss: 0.0382 - val_accuracy: 0.9489\n",
            "Epoch 195/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0374 - accuracy: 0.9505 - val_loss: 0.0381 - val_accuracy: 0.9490\n",
            "Epoch 196/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0374 - accuracy: 0.9505 - val_loss: 0.0381 - val_accuracy: 0.9491\n",
            "Epoch 197/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0374 - accuracy: 0.9506 - val_loss: 0.0381 - val_accuracy: 0.9489\n",
            "Epoch 198/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0373 - accuracy: 0.9506 - val_loss: 0.0380 - val_accuracy: 0.9491\n",
            "Epoch 199/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0373 - accuracy: 0.9507 - val_loss: 0.0380 - val_accuracy: 0.9492\n",
            "Epoch 200/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0373 - accuracy: 0.9507 - val_loss: 0.0380 - val_accuracy: 0.9492\n",
            "Epoch 201/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0372 - accuracy: 0.9508 - val_loss: 0.0379 - val_accuracy: 0.9492\n",
            "Epoch 202/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0372 - accuracy: 0.9508 - val_loss: 0.0379 - val_accuracy: 0.9493\n",
            "Epoch 203/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0372 - accuracy: 0.9508 - val_loss: 0.0379 - val_accuracy: 0.9493\n",
            "Epoch 204/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0371 - accuracy: 0.9509 - val_loss: 0.0378 - val_accuracy: 0.9493\n",
            "Epoch 205/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0371 - accuracy: 0.9510 - val_loss: 0.0378 - val_accuracy: 0.9494\n",
            "Epoch 206/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0371 - accuracy: 0.9510 - val_loss: 0.0378 - val_accuracy: 0.9494\n",
            "Epoch 207/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0370 - accuracy: 0.9510 - val_loss: 0.0377 - val_accuracy: 0.9496\n",
            "Epoch 208/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0370 - accuracy: 0.9511 - val_loss: 0.0377 - val_accuracy: 0.9496\n",
            "Epoch 209/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0370 - accuracy: 0.9511 - val_loss: 0.0377 - val_accuracy: 0.9496\n",
            "Epoch 210/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0369 - accuracy: 0.9512 - val_loss: 0.0376 - val_accuracy: 0.9496\n",
            "Epoch 211/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0369 - accuracy: 0.9512 - val_loss: 0.0376 - val_accuracy: 0.9497\n",
            "Epoch 212/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0369 - accuracy: 0.9513 - val_loss: 0.0376 - val_accuracy: 0.9497\n",
            "Epoch 213/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0368 - accuracy: 0.9513 - val_loss: 0.0375 - val_accuracy: 0.9496\n",
            "Epoch 214/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0368 - accuracy: 0.9513 - val_loss: 0.0375 - val_accuracy: 0.9497\n",
            "Epoch 215/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0368 - accuracy: 0.9513 - val_loss: 0.0375 - val_accuracy: 0.9498\n",
            "Epoch 216/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0367 - accuracy: 0.9515 - val_loss: 0.0374 - val_accuracy: 0.9499\n",
            "Epoch 217/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0367 - accuracy: 0.9514 - val_loss: 0.0374 - val_accuracy: 0.9498\n",
            "Epoch 218/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0367 - accuracy: 0.9515 - val_loss: 0.0374 - val_accuracy: 0.9499\n",
            "Epoch 219/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0366 - accuracy: 0.9516 - val_loss: 0.0373 - val_accuracy: 0.9499\n",
            "Epoch 220/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0366 - accuracy: 0.9517 - val_loss: 0.0373 - val_accuracy: 0.9499\n",
            "Epoch 221/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0366 - accuracy: 0.9516 - val_loss: 0.0373 - val_accuracy: 0.9500\n",
            "Epoch 222/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0366 - accuracy: 0.9517 - val_loss: 0.0372 - val_accuracy: 0.9501\n",
            "Epoch 223/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0365 - accuracy: 0.9517 - val_loss: 0.0372 - val_accuracy: 0.9501\n",
            "Epoch 224/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0365 - accuracy: 0.9517 - val_loss: 0.0372 - val_accuracy: 0.9501\n",
            "Epoch 225/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0365 - accuracy: 0.9519 - val_loss: 0.0371 - val_accuracy: 0.9501\n",
            "Epoch 226/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0364 - accuracy: 0.9518 - val_loss: 0.0371 - val_accuracy: 0.9503\n",
            "Epoch 227/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0364 - accuracy: 0.9518 - val_loss: 0.0371 - val_accuracy: 0.9503\n",
            "Epoch 228/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0364 - accuracy: 0.9520 - val_loss: 0.0371 - val_accuracy: 0.9504\n",
            "Epoch 229/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0363 - accuracy: 0.9519 - val_loss: 0.0370 - val_accuracy: 0.9504\n",
            "Epoch 230/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0363 - accuracy: 0.9520 - val_loss: 0.0370 - val_accuracy: 0.9504\n",
            "Epoch 231/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0363 - accuracy: 0.9521 - val_loss: 0.0370 - val_accuracy: 0.9505\n",
            "Epoch 232/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0362 - accuracy: 0.9521 - val_loss: 0.0369 - val_accuracy: 0.9505\n",
            "Epoch 233/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0362 - accuracy: 0.9521 - val_loss: 0.0369 - val_accuracy: 0.9505\n",
            "Epoch 234/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0362 - accuracy: 0.9521 - val_loss: 0.0369 - val_accuracy: 0.9505\n",
            "Epoch 235/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0362 - accuracy: 0.9522 - val_loss: 0.0368 - val_accuracy: 0.9506\n",
            "Epoch 236/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0361 - accuracy: 0.9522 - val_loss: 0.0368 - val_accuracy: 0.9507\n",
            "Epoch 237/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0361 - accuracy: 0.9522 - val_loss: 0.0368 - val_accuracy: 0.9507\n",
            "Epoch 238/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0361 - accuracy: 0.9522 - val_loss: 0.0367 - val_accuracy: 0.9508\n",
            "Epoch 239/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0360 - accuracy: 0.9523 - val_loss: 0.0367 - val_accuracy: 0.9507\n",
            "Epoch 240/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0360 - accuracy: 0.9524 - val_loss: 0.0367 - val_accuracy: 0.9508\n",
            "Epoch 241/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0360 - accuracy: 0.9523 - val_loss: 0.0366 - val_accuracy: 0.9509\n",
            "Epoch 242/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0360 - accuracy: 0.9524 - val_loss: 0.0366 - val_accuracy: 0.9509\n",
            "Epoch 243/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0359 - accuracy: 0.9524 - val_loss: 0.0366 - val_accuracy: 0.9509\n",
            "Epoch 244/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0359 - accuracy: 0.9524 - val_loss: 0.0366 - val_accuracy: 0.9509\n",
            "Epoch 245/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0359 - accuracy: 0.9525 - val_loss: 0.0365 - val_accuracy: 0.9511\n",
            "Epoch 246/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0358 - accuracy: 0.9525 - val_loss: 0.0365 - val_accuracy: 0.9511\n",
            "Epoch 247/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0358 - accuracy: 0.9525 - val_loss: 0.0365 - val_accuracy: 0.9512\n",
            "Epoch 248/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0358 - accuracy: 0.9526 - val_loss: 0.0364 - val_accuracy: 0.9512\n",
            "Epoch 249/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0358 - accuracy: 0.9526 - val_loss: 0.0364 - val_accuracy: 0.9513\n",
            "Epoch 250/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0357 - accuracy: 0.9526 - val_loss: 0.0364 - val_accuracy: 0.9514\n",
            "Epoch 251/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0357 - accuracy: 0.9527 - val_loss: 0.0363 - val_accuracy: 0.9513\n",
            "Epoch 252/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0357 - accuracy: 0.9527 - val_loss: 0.0363 - val_accuracy: 0.9514\n",
            "Epoch 253/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0356 - accuracy: 0.9527 - val_loss: 0.0363 - val_accuracy: 0.9514\n",
            "Epoch 254/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0356 - accuracy: 0.9527 - val_loss: 0.0363 - val_accuracy: 0.9516\n",
            "Epoch 255/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0356 - accuracy: 0.9528 - val_loss: 0.0362 - val_accuracy: 0.9516\n",
            "Epoch 256/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0356 - accuracy: 0.9529 - val_loss: 0.0362 - val_accuracy: 0.9517\n",
            "Epoch 257/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0355 - accuracy: 0.9529 - val_loss: 0.0362 - val_accuracy: 0.9517\n",
            "Epoch 258/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0355 - accuracy: 0.9530 - val_loss: 0.0361 - val_accuracy: 0.9519\n",
            "Epoch 259/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0355 - accuracy: 0.9530 - val_loss: 0.0361 - val_accuracy: 0.9519\n",
            "Epoch 260/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0355 - accuracy: 0.9530 - val_loss: 0.0361 - val_accuracy: 0.9519\n",
            "Epoch 261/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0354 - accuracy: 0.9531 - val_loss: 0.0361 - val_accuracy: 0.9520\n",
            "Epoch 262/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0354 - accuracy: 0.9530 - val_loss: 0.0360 - val_accuracy: 0.9520\n",
            "Epoch 263/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0354 - accuracy: 0.9531 - val_loss: 0.0360 - val_accuracy: 0.9521\n",
            "Epoch 264/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0353 - accuracy: 0.9532 - val_loss: 0.0360 - val_accuracy: 0.9521\n",
            "Epoch 265/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0353 - accuracy: 0.9533 - val_loss: 0.0359 - val_accuracy: 0.9521\n",
            "Epoch 266/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0353 - accuracy: 0.9533 - val_loss: 0.0359 - val_accuracy: 0.9521\n",
            "Epoch 267/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0353 - accuracy: 0.9534 - val_loss: 0.0359 - val_accuracy: 0.9522\n",
            "Epoch 268/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0352 - accuracy: 0.9534 - val_loss: 0.0359 - val_accuracy: 0.9522\n",
            "Epoch 269/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0352 - accuracy: 0.9534 - val_loss: 0.0358 - val_accuracy: 0.9522\n",
            "Epoch 270/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0352 - accuracy: 0.9535 - val_loss: 0.0358 - val_accuracy: 0.9522\n",
            "Epoch 271/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0352 - accuracy: 0.9534 - val_loss: 0.0358 - val_accuracy: 0.9523\n",
            "Epoch 272/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0351 - accuracy: 0.9535 - val_loss: 0.0358 - val_accuracy: 0.9525\n",
            "Epoch 273/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0351 - accuracy: 0.9535 - val_loss: 0.0357 - val_accuracy: 0.9525\n",
            "Epoch 274/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0351 - accuracy: 0.9536 - val_loss: 0.0357 - val_accuracy: 0.9524\n",
            "Epoch 275/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0351 - accuracy: 0.9536 - val_loss: 0.0357 - val_accuracy: 0.9524\n",
            "Epoch 276/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0350 - accuracy: 0.9536 - val_loss: 0.0357 - val_accuracy: 0.9525\n",
            "Epoch 277/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0350 - accuracy: 0.9537 - val_loss: 0.0356 - val_accuracy: 0.9526\n",
            "Epoch 278/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0350 - accuracy: 0.9537 - val_loss: 0.0356 - val_accuracy: 0.9524\n",
            "Epoch 279/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0350 - accuracy: 0.9537 - val_loss: 0.0356 - val_accuracy: 0.9526\n",
            "Epoch 280/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0349 - accuracy: 0.9537 - val_loss: 0.0355 - val_accuracy: 0.9526\n",
            "Epoch 281/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0349 - accuracy: 0.9538 - val_loss: 0.0355 - val_accuracy: 0.9527\n",
            "Epoch 282/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0349 - accuracy: 0.9539 - val_loss: 0.0355 - val_accuracy: 0.9526\n",
            "Epoch 283/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0349 - accuracy: 0.9539 - val_loss: 0.0355 - val_accuracy: 0.9526\n",
            "Epoch 284/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0348 - accuracy: 0.9540 - val_loss: 0.0354 - val_accuracy: 0.9526\n",
            "Epoch 285/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0348 - accuracy: 0.9539 - val_loss: 0.0354 - val_accuracy: 0.9527\n",
            "Epoch 286/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0348 - accuracy: 0.9540 - val_loss: 0.0354 - val_accuracy: 0.9527\n",
            "Epoch 287/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0348 - accuracy: 0.9540 - val_loss: 0.0354 - val_accuracy: 0.9528\n",
            "Epoch 288/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0347 - accuracy: 0.9541 - val_loss: 0.0353 - val_accuracy: 0.9528\n",
            "Epoch 289/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0347 - accuracy: 0.9541 - val_loss: 0.0353 - val_accuracy: 0.9528\n",
            "Epoch 290/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0347 - accuracy: 0.9541 - val_loss: 0.0353 - val_accuracy: 0.9529\n",
            "Epoch 291/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0347 - accuracy: 0.9542 - val_loss: 0.0353 - val_accuracy: 0.9529\n",
            "Epoch 292/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0346 - accuracy: 0.9543 - val_loss: 0.0352 - val_accuracy: 0.9530\n",
            "Epoch 293/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0346 - accuracy: 0.9543 - val_loss: 0.0352 - val_accuracy: 0.9530\n",
            "Epoch 294/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0346 - accuracy: 0.9543 - val_loss: 0.0352 - val_accuracy: 0.9530\n",
            "Epoch 295/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0346 - accuracy: 0.9543 - val_loss: 0.0352 - val_accuracy: 0.9531\n",
            "Epoch 296/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0346 - accuracy: 0.9543 - val_loss: 0.0351 - val_accuracy: 0.9531\n",
            "Epoch 297/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0345 - accuracy: 0.9544 - val_loss: 0.0351 - val_accuracy: 0.9531\n",
            "Epoch 298/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0345 - accuracy: 0.9544 - val_loss: 0.0351 - val_accuracy: 0.9532\n",
            "Epoch 299/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0345 - accuracy: 0.9545 - val_loss: 0.0351 - val_accuracy: 0.9533\n",
            "Epoch 300/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0345 - accuracy: 0.9545 - val_loss: 0.0350 - val_accuracy: 0.9532\n",
            "Epoch 301/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0344 - accuracy: 0.9546 - val_loss: 0.0350 - val_accuracy: 0.9533\n",
            "Epoch 302/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0344 - accuracy: 0.9546 - val_loss: 0.0350 - val_accuracy: 0.9534\n",
            "Epoch 303/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0344 - accuracy: 0.9546 - val_loss: 0.0350 - val_accuracy: 0.9534\n",
            "Epoch 304/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0344 - accuracy: 0.9546 - val_loss: 0.0350 - val_accuracy: 0.9534\n",
            "Epoch 305/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0344 - accuracy: 0.9546 - val_loss: 0.0349 - val_accuracy: 0.9534\n",
            "Epoch 306/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0343 - accuracy: 0.9547 - val_loss: 0.0349 - val_accuracy: 0.9536\n",
            "Epoch 307/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0343 - accuracy: 0.9546 - val_loss: 0.0349 - val_accuracy: 0.9535\n",
            "Epoch 308/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0343 - accuracy: 0.9547 - val_loss: 0.0349 - val_accuracy: 0.9537\n",
            "Epoch 309/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0343 - accuracy: 0.9547 - val_loss: 0.0348 - val_accuracy: 0.9536\n",
            "Epoch 310/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0342 - accuracy: 0.9548 - val_loss: 0.0348 - val_accuracy: 0.9536\n",
            "Epoch 311/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0342 - accuracy: 0.9548 - val_loss: 0.0348 - val_accuracy: 0.9537\n",
            "Epoch 312/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0342 - accuracy: 0.9548 - val_loss: 0.0348 - val_accuracy: 0.9538\n",
            "Epoch 313/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0342 - accuracy: 0.9548 - val_loss: 0.0347 - val_accuracy: 0.9538\n",
            "Epoch 314/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0342 - accuracy: 0.9549 - val_loss: 0.0347 - val_accuracy: 0.9539\n",
            "Epoch 315/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0341 - accuracy: 0.9549 - val_loss: 0.0347 - val_accuracy: 0.9538\n",
            "Epoch 316/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0341 - accuracy: 0.9549 - val_loss: 0.0347 - val_accuracy: 0.9538\n",
            "Epoch 317/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0341 - accuracy: 0.9550 - val_loss: 0.0347 - val_accuracy: 0.9539\n",
            "Epoch 318/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0341 - accuracy: 0.9549 - val_loss: 0.0346 - val_accuracy: 0.9540\n",
            "Epoch 319/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0341 - accuracy: 0.9550 - val_loss: 0.0346 - val_accuracy: 0.9540\n",
            "Epoch 320/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0340 - accuracy: 0.9550 - val_loss: 0.0346 - val_accuracy: 0.9540\n",
            "Epoch 321/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0340 - accuracy: 0.9550 - val_loss: 0.0346 - val_accuracy: 0.9541\n",
            "Epoch 322/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0340 - accuracy: 0.9550 - val_loss: 0.0345 - val_accuracy: 0.9541\n",
            "Epoch 323/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0340 - accuracy: 0.9551 - val_loss: 0.0345 - val_accuracy: 0.9541\n",
            "Epoch 324/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0340 - accuracy: 0.9551 - val_loss: 0.0345 - val_accuracy: 0.9542\n",
            "Epoch 325/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0339 - accuracy: 0.9552 - val_loss: 0.0345 - val_accuracy: 0.9542\n",
            "Epoch 326/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0339 - accuracy: 0.9552 - val_loss: 0.0345 - val_accuracy: 0.9543\n",
            "Epoch 327/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0339 - accuracy: 0.9552 - val_loss: 0.0344 - val_accuracy: 0.9544\n",
            "Epoch 328/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0339 - accuracy: 0.9552 - val_loss: 0.0344 - val_accuracy: 0.9542\n",
            "Epoch 329/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0339 - accuracy: 0.9552 - val_loss: 0.0344 - val_accuracy: 0.9544\n",
            "Epoch 330/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0338 - accuracy: 0.9553 - val_loss: 0.0344 - val_accuracy: 0.9543\n",
            "Epoch 331/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0338 - accuracy: 0.9553 - val_loss: 0.0344 - val_accuracy: 0.9544\n",
            "Epoch 332/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0338 - accuracy: 0.9553 - val_loss: 0.0343 - val_accuracy: 0.9544\n",
            "Epoch 333/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0338 - accuracy: 0.9554 - val_loss: 0.0343 - val_accuracy: 0.9544\n",
            "Epoch 334/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0338 - accuracy: 0.9554 - val_loss: 0.0343 - val_accuracy: 0.9545\n",
            "Epoch 335/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0337 - accuracy: 0.9554 - val_loss: 0.0343 - val_accuracy: 0.9546\n",
            "Epoch 336/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0337 - accuracy: 0.9553 - val_loss: 0.0343 - val_accuracy: 0.9546\n",
            "Epoch 337/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0337 - accuracy: 0.9554 - val_loss: 0.0342 - val_accuracy: 0.9546\n",
            "Epoch 338/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0337 - accuracy: 0.9554 - val_loss: 0.0342 - val_accuracy: 0.9546\n",
            "Epoch 339/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0337 - accuracy: 0.9554 - val_loss: 0.0342 - val_accuracy: 0.9548\n",
            "Epoch 340/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0336 - accuracy: 0.9554 - val_loss: 0.0342 - val_accuracy: 0.9548\n",
            "Epoch 341/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0336 - accuracy: 0.9555 - val_loss: 0.0342 - val_accuracy: 0.9549\n",
            "Epoch 342/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0336 - accuracy: 0.9555 - val_loss: 0.0341 - val_accuracy: 0.9549\n",
            "Epoch 343/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0336 - accuracy: 0.9555 - val_loss: 0.0341 - val_accuracy: 0.9549\n",
            "Epoch 344/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0336 - accuracy: 0.9555 - val_loss: 0.0341 - val_accuracy: 0.9549\n",
            "Epoch 345/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0336 - accuracy: 0.9556 - val_loss: 0.0341 - val_accuracy: 0.9550\n",
            "Epoch 346/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0335 - accuracy: 0.9556 - val_loss: 0.0341 - val_accuracy: 0.9550\n",
            "Epoch 347/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0335 - accuracy: 0.9556 - val_loss: 0.0340 - val_accuracy: 0.9550\n",
            "Epoch 348/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0335 - accuracy: 0.9556 - val_loss: 0.0340 - val_accuracy: 0.9551\n",
            "Epoch 349/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0335 - accuracy: 0.9557 - val_loss: 0.0340 - val_accuracy: 0.9551\n",
            "Epoch 350/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0335 - accuracy: 0.9557 - val_loss: 0.0340 - val_accuracy: 0.9551\n",
            "Epoch 351/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0334 - accuracy: 0.9556 - val_loss: 0.0340 - val_accuracy: 0.9551\n",
            "Epoch 352/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0334 - accuracy: 0.9557 - val_loss: 0.0340 - val_accuracy: 0.9553\n",
            "Epoch 353/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0334 - accuracy: 0.9557 - val_loss: 0.0339 - val_accuracy: 0.9552\n",
            "Epoch 354/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0334 - accuracy: 0.9557 - val_loss: 0.0339 - val_accuracy: 0.9552\n",
            "Epoch 355/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0334 - accuracy: 0.9557 - val_loss: 0.0339 - val_accuracy: 0.9553\n",
            "Epoch 356/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0334 - accuracy: 0.9559 - val_loss: 0.0339 - val_accuracy: 0.9553\n",
            "Epoch 357/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0333 - accuracy: 0.9558 - val_loss: 0.0339 - val_accuracy: 0.9553\n",
            "Epoch 358/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0333 - accuracy: 0.9559 - val_loss: 0.0338 - val_accuracy: 0.9554\n",
            "Epoch 359/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0333 - accuracy: 0.9559 - val_loss: 0.0338 - val_accuracy: 0.9554\n",
            "Epoch 360/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0333 - accuracy: 0.9559 - val_loss: 0.0338 - val_accuracy: 0.9554\n",
            "Epoch 361/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0333 - accuracy: 0.9559 - val_loss: 0.0338 - val_accuracy: 0.9553\n",
            "Epoch 362/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0333 - accuracy: 0.9559 - val_loss: 0.0338 - val_accuracy: 0.9555\n",
            "Epoch 363/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0332 - accuracy: 0.9560 - val_loss: 0.0338 - val_accuracy: 0.9555\n",
            "Epoch 364/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0332 - accuracy: 0.9560 - val_loss: 0.0337 - val_accuracy: 0.9555\n",
            "Epoch 365/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0332 - accuracy: 0.9560 - val_loss: 0.0337 - val_accuracy: 0.9556\n",
            "Epoch 366/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0332 - accuracy: 0.9560 - val_loss: 0.0337 - val_accuracy: 0.9556\n",
            "Epoch 367/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0332 - accuracy: 0.9561 - val_loss: 0.0337 - val_accuracy: 0.9556\n",
            "Epoch 368/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0332 - accuracy: 0.9560 - val_loss: 0.0337 - val_accuracy: 0.9557\n",
            "Epoch 369/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0331 - accuracy: 0.9561 - val_loss: 0.0337 - val_accuracy: 0.9556\n",
            "Epoch 370/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0331 - accuracy: 0.9561 - val_loss: 0.0336 - val_accuracy: 0.9557\n",
            "Epoch 371/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0331 - accuracy: 0.9561 - val_loss: 0.0336 - val_accuracy: 0.9558\n",
            "Epoch 372/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0331 - accuracy: 0.9561 - val_loss: 0.0336 - val_accuracy: 0.9558\n",
            "Epoch 373/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0331 - accuracy: 0.9562 - val_loss: 0.0336 - val_accuracy: 0.9558\n",
            "Epoch 374/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0331 - accuracy: 0.9562 - val_loss: 0.0336 - val_accuracy: 0.9558\n",
            "Epoch 375/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0331 - accuracy: 0.9562 - val_loss: 0.0336 - val_accuracy: 0.9558\n",
            "Epoch 376/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0330 - accuracy: 0.9562 - val_loss: 0.0335 - val_accuracy: 0.9558\n",
            "Epoch 377/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0330 - accuracy: 0.9562 - val_loss: 0.0335 - val_accuracy: 0.9558\n",
            "Epoch 378/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0330 - accuracy: 0.9562 - val_loss: 0.0335 - val_accuracy: 0.9559\n",
            "Epoch 379/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0330 - accuracy: 0.9562 - val_loss: 0.0335 - val_accuracy: 0.9560\n",
            "Epoch 380/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0330 - accuracy: 0.9563 - val_loss: 0.0335 - val_accuracy: 0.9559\n",
            "Epoch 381/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0330 - accuracy: 0.9563 - val_loss: 0.0335 - val_accuracy: 0.9559\n",
            "Epoch 382/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0329 - accuracy: 0.9564 - val_loss: 0.0334 - val_accuracy: 0.9559\n",
            "Epoch 383/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0329 - accuracy: 0.9564 - val_loss: 0.0334 - val_accuracy: 0.9559\n",
            "Epoch 384/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0329 - accuracy: 0.9564 - val_loss: 0.0334 - val_accuracy: 0.9559\n",
            "Epoch 385/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0329 - accuracy: 0.9564 - val_loss: 0.0334 - val_accuracy: 0.9560\n",
            "Epoch 386/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0329 - accuracy: 0.9564 - val_loss: 0.0334 - val_accuracy: 0.9560\n",
            "Epoch 387/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0329 - accuracy: 0.9564 - val_loss: 0.0334 - val_accuracy: 0.9561\n",
            "Epoch 388/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0329 - accuracy: 0.9564 - val_loss: 0.0334 - val_accuracy: 0.9561\n",
            "Epoch 389/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0328 - accuracy: 0.9564 - val_loss: 0.0333 - val_accuracy: 0.9561\n",
            "Epoch 390/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0328 - accuracy: 0.9565 - val_loss: 0.0333 - val_accuracy: 0.9561\n",
            "Epoch 391/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0328 - accuracy: 0.9566 - val_loss: 0.0333 - val_accuracy: 0.9561\n",
            "Epoch 392/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0328 - accuracy: 0.9565 - val_loss: 0.0333 - val_accuracy: 0.9561\n",
            "Epoch 393/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0328 - accuracy: 0.9566 - val_loss: 0.0333 - val_accuracy: 0.9562\n",
            "Epoch 394/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0328 - accuracy: 0.9566 - val_loss: 0.0333 - val_accuracy: 0.9561\n",
            "Epoch 395/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0328 - accuracy: 0.9566 - val_loss: 0.0332 - val_accuracy: 0.9562\n",
            "Epoch 396/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0327 - accuracy: 0.9566 - val_loss: 0.0332 - val_accuracy: 0.9562\n",
            "Epoch 397/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0327 - accuracy: 0.9566 - val_loss: 0.0332 - val_accuracy: 0.9563\n",
            "Epoch 398/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0327 - accuracy: 0.9567 - val_loss: 0.0332 - val_accuracy: 0.9563\n",
            "Epoch 399/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0327 - accuracy: 0.9567 - val_loss: 0.0332 - val_accuracy: 0.9562\n",
            "Epoch 400/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0327 - accuracy: 0.9567 - val_loss: 0.0332 - val_accuracy: 0.9563\n",
            "Epoch 401/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0327 - accuracy: 0.9567 - val_loss: 0.0332 - val_accuracy: 0.9563\n",
            "Epoch 402/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0327 - accuracy: 0.9567 - val_loss: 0.0331 - val_accuracy: 0.9563\n",
            "Epoch 403/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0327 - accuracy: 0.9568 - val_loss: 0.0331 - val_accuracy: 0.9563\n",
            "Epoch 404/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0326 - accuracy: 0.9568 - val_loss: 0.0331 - val_accuracy: 0.9563\n",
            "Epoch 405/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0326 - accuracy: 0.9568 - val_loss: 0.0331 - val_accuracy: 0.9564\n",
            "Epoch 406/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0326 - accuracy: 0.9568 - val_loss: 0.0331 - val_accuracy: 0.9564\n",
            "Epoch 407/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0326 - accuracy: 0.9568 - val_loss: 0.0331 - val_accuracy: 0.9565\n",
            "Epoch 408/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0326 - accuracy: 0.9568 - val_loss: 0.0331 - val_accuracy: 0.9565\n",
            "Epoch 409/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0326 - accuracy: 0.9569 - val_loss: 0.0331 - val_accuracy: 0.9565\n",
            "Epoch 410/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0326 - accuracy: 0.9569 - val_loss: 0.0330 - val_accuracy: 0.9565\n",
            "Epoch 411/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0325 - accuracy: 0.9569 - val_loss: 0.0330 - val_accuracy: 0.9566\n",
            "Epoch 412/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0325 - accuracy: 0.9570 - val_loss: 0.0330 - val_accuracy: 0.9565\n",
            "Epoch 413/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0325 - accuracy: 0.9569 - val_loss: 0.0330 - val_accuracy: 0.9566\n",
            "Epoch 414/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0325 - accuracy: 0.9569 - val_loss: 0.0330 - val_accuracy: 0.9566\n",
            "Epoch 415/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0325 - accuracy: 0.9569 - val_loss: 0.0330 - val_accuracy: 0.9566\n",
            "Epoch 416/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0325 - accuracy: 0.9570 - val_loss: 0.0330 - val_accuracy: 0.9566\n",
            "Epoch 417/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0325 - accuracy: 0.9571 - val_loss: 0.0329 - val_accuracy: 0.9567\n",
            "Epoch 418/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0325 - accuracy: 0.9570 - val_loss: 0.0329 - val_accuracy: 0.9567\n",
            "Epoch 419/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0325 - accuracy: 0.9569 - val_loss: 0.0329 - val_accuracy: 0.9567\n",
            "Epoch 420/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0324 - accuracy: 0.9570 - val_loss: 0.0329 - val_accuracy: 0.9567\n",
            "Epoch 421/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0324 - accuracy: 0.9570 - val_loss: 0.0329 - val_accuracy: 0.9568\n",
            "Epoch 422/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0324 - accuracy: 0.9570 - val_loss: 0.0329 - val_accuracy: 0.9568\n",
            "Epoch 423/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0324 - accuracy: 0.9571 - val_loss: 0.0329 - val_accuracy: 0.9567\n",
            "Epoch 424/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0324 - accuracy: 0.9571 - val_loss: 0.0329 - val_accuracy: 0.9568\n",
            "Epoch 425/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0324 - accuracy: 0.9571 - val_loss: 0.0329 - val_accuracy: 0.9567\n",
            "Epoch 426/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0324 - accuracy: 0.9571 - val_loss: 0.0328 - val_accuracy: 0.9569\n",
            "Epoch 427/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0324 - accuracy: 0.9571 - val_loss: 0.0328 - val_accuracy: 0.9569\n",
            "Epoch 428/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0323 - accuracy: 0.9571 - val_loss: 0.0328 - val_accuracy: 0.9568\n",
            "Epoch 429/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0323 - accuracy: 0.9572 - val_loss: 0.0328 - val_accuracy: 0.9569\n",
            "Epoch 430/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0323 - accuracy: 0.9571 - val_loss: 0.0328 - val_accuracy: 0.9569\n",
            "Epoch 431/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0323 - accuracy: 0.9572 - val_loss: 0.0328 - val_accuracy: 0.9568\n",
            "Epoch 432/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0323 - accuracy: 0.9572 - val_loss: 0.0328 - val_accuracy: 0.9568\n",
            "Epoch 433/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0323 - accuracy: 0.9571 - val_loss: 0.0328 - val_accuracy: 0.9569\n",
            "Epoch 434/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0323 - accuracy: 0.9572 - val_loss: 0.0327 - val_accuracy: 0.9569\n",
            "Epoch 435/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0323 - accuracy: 0.9572 - val_loss: 0.0327 - val_accuracy: 0.9568\n",
            "Epoch 436/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0323 - accuracy: 0.9573 - val_loss: 0.0327 - val_accuracy: 0.9569\n",
            "Epoch 437/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0322 - accuracy: 0.9572 - val_loss: 0.0327 - val_accuracy: 0.9569\n",
            "Epoch 438/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0322 - accuracy: 0.9572 - val_loss: 0.0327 - val_accuracy: 0.9570\n",
            "Epoch 439/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0322 - accuracy: 0.9573 - val_loss: 0.0327 - val_accuracy: 0.9568\n",
            "Epoch 440/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0322 - accuracy: 0.9573 - val_loss: 0.0327 - val_accuracy: 0.9569\n",
            "Epoch 441/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0322 - accuracy: 0.9573 - val_loss: 0.0327 - val_accuracy: 0.9569\n",
            "Epoch 442/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0322 - accuracy: 0.9573 - val_loss: 0.0326 - val_accuracy: 0.9570\n",
            "Epoch 443/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0322 - accuracy: 0.9573 - val_loss: 0.0326 - val_accuracy: 0.9570\n",
            "Epoch 444/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0322 - accuracy: 0.9573 - val_loss: 0.0326 - val_accuracy: 0.9570\n",
            "Epoch 445/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0322 - accuracy: 0.9575 - val_loss: 0.0326 - val_accuracy: 0.9570\n",
            "Epoch 446/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0321 - accuracy: 0.9574 - val_loss: 0.0326 - val_accuracy: 0.9569\n",
            "Epoch 447/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0321 - accuracy: 0.9574 - val_loss: 0.0326 - val_accuracy: 0.9570\n",
            "Epoch 448/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0321 - accuracy: 0.9574 - val_loss: 0.0326 - val_accuracy: 0.9571\n",
            "Epoch 449/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0321 - accuracy: 0.9574 - val_loss: 0.0326 - val_accuracy: 0.9571\n",
            "Epoch 450/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0321 - accuracy: 0.9575 - val_loss: 0.0326 - val_accuracy: 0.9571\n",
            "Epoch 451/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0321 - accuracy: 0.9575 - val_loss: 0.0325 - val_accuracy: 0.9571\n",
            "Epoch 452/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0321 - accuracy: 0.9575 - val_loss: 0.0325 - val_accuracy: 0.9571\n",
            "Epoch 453/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0321 - accuracy: 0.9575 - val_loss: 0.0325 - val_accuracy: 0.9571\n",
            "Epoch 454/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0321 - accuracy: 0.9575 - val_loss: 0.0325 - val_accuracy: 0.9571\n",
            "Epoch 455/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0320 - accuracy: 0.9576 - val_loss: 0.0325 - val_accuracy: 0.9572\n",
            "Epoch 456/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0320 - accuracy: 0.9575 - val_loss: 0.0325 - val_accuracy: 0.9571\n",
            "Epoch 457/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0320 - accuracy: 0.9576 - val_loss: 0.0325 - val_accuracy: 0.9572\n",
            "Epoch 458/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0320 - accuracy: 0.9576 - val_loss: 0.0325 - val_accuracy: 0.9572\n",
            "Epoch 459/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0320 - accuracy: 0.9577 - val_loss: 0.0325 - val_accuracy: 0.9571\n",
            "Epoch 460/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0320 - accuracy: 0.9575 - val_loss: 0.0325 - val_accuracy: 0.9572\n",
            "Epoch 461/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0320 - accuracy: 0.9577 - val_loss: 0.0324 - val_accuracy: 0.9572\n",
            "Epoch 462/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0320 - accuracy: 0.9577 - val_loss: 0.0324 - val_accuracy: 0.9571\n",
            "Epoch 463/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0320 - accuracy: 0.9577 - val_loss: 0.0324 - val_accuracy: 0.9572\n",
            "Epoch 464/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0320 - accuracy: 0.9577 - val_loss: 0.0324 - val_accuracy: 0.9571\n",
            "Epoch 465/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0319 - accuracy: 0.9577 - val_loss: 0.0324 - val_accuracy: 0.9572\n",
            "Epoch 466/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0319 - accuracy: 0.9577 - val_loss: 0.0324 - val_accuracy: 0.9572\n",
            "Epoch 467/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0319 - accuracy: 0.9577 - val_loss: 0.0324 - val_accuracy: 0.9572\n",
            "Epoch 468/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0319 - accuracy: 0.9577 - val_loss: 0.0324 - val_accuracy: 0.9573\n",
            "Epoch 469/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0319 - accuracy: 0.9577 - val_loss: 0.0324 - val_accuracy: 0.9572\n",
            "Epoch 470/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0319 - accuracy: 0.9578 - val_loss: 0.0324 - val_accuracy: 0.9572\n",
            "Epoch 471/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0319 - accuracy: 0.9577 - val_loss: 0.0323 - val_accuracy: 0.9572\n",
            "Epoch 472/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0319 - accuracy: 0.9578 - val_loss: 0.0323 - val_accuracy: 0.9572\n",
            "Epoch 473/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0319 - accuracy: 0.9578 - val_loss: 0.0323 - val_accuracy: 0.9572\n",
            "Epoch 474/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0319 - accuracy: 0.9578 - val_loss: 0.0323 - val_accuracy: 0.9572\n",
            "Epoch 475/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0319 - accuracy: 0.9578 - val_loss: 0.0323 - val_accuracy: 0.9572\n",
            "Epoch 476/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0318 - accuracy: 0.9578 - val_loss: 0.0323 - val_accuracy: 0.9572\n",
            "Epoch 477/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0318 - accuracy: 0.9578 - val_loss: 0.0323 - val_accuracy: 0.9573\n",
            "Epoch 478/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0318 - accuracy: 0.9578 - val_loss: 0.0323 - val_accuracy: 0.9572\n",
            "Epoch 479/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0318 - accuracy: 0.9578 - val_loss: 0.0323 - val_accuracy: 0.9572\n",
            "Epoch 480/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0318 - accuracy: 0.9579 - val_loss: 0.0323 - val_accuracy: 0.9573\n",
            "Epoch 481/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0318 - accuracy: 0.9579 - val_loss: 0.0322 - val_accuracy: 0.9573\n",
            "Epoch 482/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0318 - accuracy: 0.9579 - val_loss: 0.0322 - val_accuracy: 0.9572\n",
            "Epoch 483/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0318 - accuracy: 0.9579 - val_loss: 0.0322 - val_accuracy: 0.9573\n",
            "Epoch 484/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0318 - accuracy: 0.9579 - val_loss: 0.0322 - val_accuracy: 0.9572\n",
            "Epoch 485/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0318 - accuracy: 0.9580 - val_loss: 0.0322 - val_accuracy: 0.9572\n",
            "Epoch 486/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0318 - accuracy: 0.9579 - val_loss: 0.0322 - val_accuracy: 0.9573\n",
            "Epoch 487/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0317 - accuracy: 0.9579 - val_loss: 0.0322 - val_accuracy: 0.9572\n",
            "Epoch 488/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0317 - accuracy: 0.9580 - val_loss: 0.0322 - val_accuracy: 0.9572\n",
            "Epoch 489/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0317 - accuracy: 0.9580 - val_loss: 0.0322 - val_accuracy: 0.9572\n",
            "Epoch 490/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0317 - accuracy: 0.9580 - val_loss: 0.0322 - val_accuracy: 0.9572\n",
            "Epoch 491/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0317 - accuracy: 0.9580 - val_loss: 0.0322 - val_accuracy: 0.9572\n",
            "Epoch 492/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0317 - accuracy: 0.9580 - val_loss: 0.0322 - val_accuracy: 0.9573\n",
            "Epoch 493/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0317 - accuracy: 0.9580 - val_loss: 0.0321 - val_accuracy: 0.9572\n",
            "Epoch 494/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0317 - accuracy: 0.9581 - val_loss: 0.0321 - val_accuracy: 0.9573\n",
            "Epoch 495/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0317 - accuracy: 0.9581 - val_loss: 0.0321 - val_accuracy: 0.9573\n",
            "Epoch 496/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0317 - accuracy: 0.9581 - val_loss: 0.0321 - val_accuracy: 0.9572\n",
            "Epoch 497/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0317 - accuracy: 0.9580 - val_loss: 0.0321 - val_accuracy: 0.9572\n",
            "Epoch 498/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0317 - accuracy: 0.9581 - val_loss: 0.0321 - val_accuracy: 0.9573\n",
            "Epoch 499/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0316 - accuracy: 0.9581 - val_loss: 0.0321 - val_accuracy: 0.9572\n",
            "Epoch 500/500\n",
            "342/342 [==============================] - 1s 2ms/step - loss: 0.0316 - accuracy: 0.9582 - val_loss: 0.0321 - val_accuracy: 0.9573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdismSwjXIx3"
      },
      "source": [
        "##***2. Results and Conclusion***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvunKotmXbRj"
      },
      "source": [
        "Two plots are shown, which have the objective of representing the results obtained regarding the training process and the testing process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuTgas1_FtT8",
        "outputId": "5935a74f-6364-42d1-c2fd-7efdaf34445d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(history.history.keys())\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy6uq-qPbDTl"
      },
      "source": [
        "***Results:*** In the first plot, you can see the increasing tendency of the accuracy both in the training process and in the testing process. It is observed that the tendency of the accuracy is the same for each specified process. In that sense, it is established that there is no overfitting. Similarly, the accuracy reaches values higher than ***95%***. In the second plot, the decreasing tendency of the loss is observed. In this case, as can be seen in the plot, the loss in training has the same behavior as the loss in the test. In addition, loss values lower than ***4%*** are achieved\n",
        "\n",
        "***Conclusion:*** In short, it was possible to read and describe the data respectively. In this case, the data deals with the detection of the Higgs Boson in simulated data to reproduce the behavior of the ATLAS experience. The data was preprocessed using cleaning, transformation and feature selection. In that sense, the application of a deep learning model was possible. Exceptional results were obtained with an accuracy greater than ***95%*** and a loss less than ***4%***."
      ]
    }
  ]
}